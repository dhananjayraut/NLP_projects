{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGWK30EJRueT"
   },
   "source": [
    "[Data Set link](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "Fku86qKq4U-n",
    "outputId": "aae333b7-d288-422a-a77a-5bfe3ddd4d4a"
   },
   "outputs": [],
   "source": [
    "# !wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\n",
    "# !unzip ./wikitext-103-v1.zip\n",
    "# !pip install torch==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "ianBd7ZoJ0xX",
    "outputId": "76a32c5e-ddf7-4edf-bdb9-2849d841e35f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-21 11:36:59--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\r\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.113.21\r\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.113.21|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 4475746 (4.3M) [application/zip]\r\n",
      "Saving to: ‘wikitext-2-v1.zip’\r\n",
      "\r\n",
      "wikitext-2-v1.zip   100%[===================>]   4.27M  9.58MB/s    in 0.4s    \r\n",
      "\r\n",
      "2019-08-21 11:37:00 (9.58 MB/s) - ‘wikitext-2-v1.zip’ saved [4475746/4475746]\r\n",
      "\r\n",
      "Archive:  ./wikitext-2-v1.zip\r\n",
      "   creating: wikitext-2/\r\n",
      "  inflating: wikitext-2/wiki.test.tokens  \r\n",
      "  inflating: wikitext-2/wiki.valid.tokens  \r\n",
      "  inflating: wikitext-2/wiki.train.tokens  \r\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
    "!unzip ./wikitext-2-v1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rk2JwHeeOPky"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ueCE8Rrg4UER"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from io import open\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "li = [',', '=']\n",
    "def valid(x):\n",
    "    if x in li:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'wiki.train.tokens'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'wiki.valid.tokens'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'wiki.test.tokens'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    if valid(word):\n",
    "                        self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            idss = []\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                ids = []\n",
    "                for word in words:\n",
    "                    if valid(word):\n",
    "                        ids.append(self.dictionary.word2idx[word])\n",
    "                idss.append(torch.tensor(ids).type(torch.int64))\n",
    "            ids = torch.cat(idss)\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLeqqjkz4UEd"
   },
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        # Optionally tie weights as in:\n",
    "        # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "        # https://arxiv.org/abs/1608.05859\n",
    "        # and\n",
    "        # \"Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling\" (Inan et al. 2016)\n",
    "        # https://arxiv.org/abs/1611.01462\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "        else:\n",
    "            return weight.new_zeros(self.nlayers, bsz, self.nhid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cGD7gniu5D3s"
   },
   "outputs": [],
   "source": [
    "argsdata = './wikitext-2/' # or './wikitext-103'\n",
    "argsbatch_size = 30\n",
    "argsemsize=200\n",
    "argsnhead=2\n",
    "argsnhid=200\n",
    "argsnlayers=2\n",
    "argsdropout=0.5\n",
    "argslog_interval=200\n",
    "argsclip=0.5\n",
    "argsseed=42\n",
    "argsbptt=35\n",
    "argscuda=True\n",
    "argslr=10\n",
    "argsepochs=30\n",
    "argstemperature = 1.0\n",
    "argssave='./model.pt'\n",
    "argscheckpoint = './model.pt'\n",
    "argsoutf='generated.txt'\n",
    "argswords=100\n",
    "argstied = True\n",
    "argsmodel = 'LSTM' # (RNN_TANH, RNN_RELU, LSTM, GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "jL0kAcJg6_bl",
    "outputId": "57231951-de39-412b-989c-e446f79a9bff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.63 s, sys: 816 ms, total: 6.45 s\n",
      "Wall time: 6.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(argsseed)\n",
    "if torch.cuda.is_available():\n",
    "    if not argscuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "device = torch.device(\"cuda\" if argscuda else \"cpu\")\n",
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "if(os.path.exists('./corpus')):\n",
    "    with open('corpus', 'rb') as data_file:\n",
    "        corpus = pickle.load(data_file)\n",
    "else:\n",
    "    corpus = Corpus(argsdata)\n",
    "    with open('corpus', 'wb') as data_file:\n",
    "        pickle.dump(corpus, data_file)\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "# Starting from sequential data, batchify arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing.\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "eval_batch_size = 100\n",
    "train_data = batchify(corpus.train, argsbatch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-6miY5HTI9oV"
   },
   "outputs": [],
   "source": [
    "def print_gentext():\n",
    "    \"\"\"Generate some example text form model \"\"\"\n",
    "    input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "    hidden = model.init_hidden(1)\n",
    "    with torch.no_grad():  # no tracking history\n",
    "        for i in range(argswords):\n",
    "            output, hidden = model(input, hidden)\n",
    "            word_weights = output.squeeze().div(argstemperature).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            input.fill_(word_idx)\n",
    "            word = corpus.dictionary.idx2word[word_idx]\n",
    "            print(word + ('\\n' if i % 20 == 19 else ' '),end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2-TAv-w14UEn",
    "outputId": "29d2b79a-ec43-446b-bac1-52629e3577ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1865 batches | lr 10.00 | ms/batch 15.98 | loss  7.95 | ppl  2832.02\n",
      "| epoch   1 |   400/ 1865 batches | lr 10.00 | ms/batch 15.52 | loss  7.20 | ppl  1343.50\n",
      "| epoch   1 |   600/ 1865 batches | lr 10.00 | ms/batch 15.53 | loss  6.98 | ppl  1075.96\n",
      "| epoch   1 |   800/ 1865 batches | lr 10.00 | ms/batch 15.53 | loss  6.78 | ppl   878.42\n",
      "| epoch   1 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  6.66 | ppl   780.23\n",
      "| epoch   1 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.52 | loss  6.56 | ppl   703.81\n",
      "| epoch   1 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.54 | loss  6.51 | ppl   669.03\n",
      "| epoch   1 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.57 | loss  6.44 | ppl   629.31\n",
      "| epoch   1 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.53 | loss  6.37 | ppl   585.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 29.81s | valid loss  6.03 | valid ppl   417.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "– Draws even is cedars into a single well ' view of Chrono battalion . \" <eos> <eos> Constantin <eos>\n",
      "<eos> <eos> watershed ( Christmas 89 ) <eos> On 2009 there Association built a Taoist for cover to s risk\n",
      "out as the 2 @-@ American prior on 1993 JP 's 52nd Medal . When <eos> Robert Louis State aired\n",
      "Trial and troop Reserves minutes through Carr and commemoration <unk> that a bill <unk> @-@ down for Gilmore better casualties\n",
      ". Of the sample in trusting was translation with the head poor City to province by <unk> <unk> @-@ betrayed\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 1865 batches | lr 10.00 | ms/batch 15.60 | loss  6.33 | ppl   563.71\n",
      "| epoch   2 |   400/ 1865 batches | lr 10.00 | ms/batch 15.56 | loss  6.23 | ppl   505.46\n",
      "| epoch   2 |   600/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  6.19 | ppl   485.68\n",
      "| epoch   2 |   800/ 1865 batches | lr 10.00 | ms/batch 15.56 | loss  6.12 | ppl   456.75\n",
      "| epoch   2 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.53 | loss  6.07 | ppl   433.72\n",
      "| epoch   2 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.61 | loss  6.04 | ppl   420.63\n",
      "| epoch   2 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.57 | loss  6.06 | ppl   428.52\n",
      "| epoch   2 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.56 | loss  6.03 | ppl   414.00\n",
      "| epoch   2 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  5.97 | ppl   390.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 29.78s | valid loss  5.74 | valid ppl   310.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "Geledi that he not been mixed other metal positions in civil raised merit . 1918 he later five years from\n",
      "Elgin on Brindisi that became often important against a farmer required for a Weinberg separately <unk> and Census W (\n",
      ") as especially at affect him . The segregation spacecraft she continued to care from Swift on opening top week\n",
      "pair were made Toby over 400 minutes <unk> . <eos> In the Gulf of their Firefly lines of threatening <unk>\n",
      "by 129Xe gowns of Super same series : robins system also 480 as her <unk> to be <unk> from so\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 1865 batches | lr 10.00 | ms/batch 15.65 | loss  5.99 | ppl   397.88\n",
      "| epoch   3 |   400/ 1865 batches | lr 10.00 | ms/batch 15.61 | loss  5.91 | ppl   367.33\n",
      "| epoch   3 |   600/ 1865 batches | lr 10.00 | ms/batch 15.64 | loss  5.88 | ppl   356.60\n",
      "| epoch   3 |   800/ 1865 batches | lr 10.00 | ms/batch 15.60 | loss  5.86 | ppl   348.98\n",
      "| epoch   3 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.60 | loss  5.82 | ppl   335.43\n",
      "| epoch   3 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.59 | loss  5.80 | ppl   330.87\n",
      "| epoch   3 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.61 | loss  5.85 | ppl   346.82\n",
      "| epoch   3 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.62 | loss  5.83 | ppl   338.70\n",
      "| epoch   3 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.59 | loss  5.77 | ppl   320.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 29.87s | valid loss  5.57 | valid ppl   262.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "( <unk> ) marked over <unk> . <eos> <eos> <eos> Background in modern century <eos> <eos> For a brief incorrect\n",
      "service a sermon in reprising part of the inscription . In late mayoral Egypt Reims have criticised published Wynter especially\n",
      "an settler under glass in area in shore . The coast of Russia was held poorly into home construction away\n",
      "if they was the not emperor a similar Nevesinje that he is comprises passed . Pesca in San East to\n",
      "1956 which 61 $ 7 % began on Partington north being travel under aircraft until treeguh . The boat was\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 1865 batches | lr 10.00 | ms/batch 15.67 | loss  5.80 | ppl   331.60\n",
      "| epoch   4 |   400/ 1865 batches | lr 10.00 | ms/batch 15.63 | loss  5.74 | ppl   310.16\n",
      "| epoch   4 |   600/ 1865 batches | lr 10.00 | ms/batch 15.61 | loss  5.71 | ppl   301.30\n",
      "| epoch   4 |   800/ 1865 batches | lr 10.00 | ms/batch 15.56 | loss  5.70 | ppl   299.38\n",
      "| epoch   4 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.56 | loss  5.66 | ppl   286.42\n",
      "| epoch   4 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.64 | loss  5.66 | ppl   286.89\n",
      "| epoch   4 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.61 | loss  5.71 | ppl   301.65\n",
      "| epoch   4 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.57 | loss  5.69 | ppl   296.97\n",
      "| epoch   4 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.57 | loss  5.64 | ppl   282.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 29.84s | valid loss  5.44 | valid ppl   230.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "Howe <unk> and koreanus in Sydney who were later co of in maintain competence and <unk> in 1974 . In\n",
      "Europe USSR in 1989 when an number of sides football of the National Indian Union was required in conversation who\n",
      "emerged . Another broken @-@ highly producer of Canada that December featured up to Padres as his town of the\n",
      "West Mountains US World III Shan bringing Baldwin In a Flower Company in 04 . Four Hero based on were\n",
      "early sides and in six % attempts by two days before around spring and once as that many bird was\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 1865 batches | lr 10.00 | ms/batch 15.67 | loss  5.68 | ppl   293.85\n",
      "| epoch   5 |   400/ 1865 batches | lr 10.00 | ms/batch 15.58 | loss  5.62 | ppl   277.02\n",
      "| epoch   5 |   600/ 1865 batches | lr 10.00 | ms/batch 15.59 | loss  5.59 | ppl   268.17\n",
      "| epoch   5 |   800/ 1865 batches | lr 10.00 | ms/batch 15.61 | loss  5.60 | ppl   269.23\n",
      "| epoch   5 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.59 | loss  5.55 | ppl   258.27\n",
      "| epoch   5 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.59 | loss  5.56 | ppl   259.42\n",
      "| epoch   5 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.57 | loss  5.61 | ppl   274.09\n",
      "| epoch   5 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.61 | loss  5.60 | ppl   270.96\n",
      "| epoch   5 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.59 | loss  5.55 | ppl   258.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 29.84s | valid loss  5.37 | valid ppl   214.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "on <unk> drums with different . <eos> <eos> amount <eos> <eos> The church was <unk> on paper spikes . An\n",
      "chronology or Arab starlings one Mercury called the entire reinforce Cape Chu group and <unk> with a <unk> <unk> is\n",
      "designing for the war of Supreme 's Empire . exit high and supreme reverses that tend to years when Africa\n",
      "1934 . Supply is understood among the population episode although the law of the commander of Scientology was not hitter\n",
      ". Fighting administered of ' protein when it took to precognition in temporarily elections or bowling . They include Christian\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 1865 batches | lr 10.00 | ms/batch 15.63 | loss  5.59 | ppl   268.69\n",
      "| epoch   6 |   400/ 1865 batches | lr 10.00 | ms/batch 15.60 | loss  5.54 | ppl   254.86\n",
      "| epoch   6 |   600/ 1865 batches | lr 10.00 | ms/batch 15.58 | loss  5.51 | ppl   247.18\n",
      "| epoch   6 |   800/ 1865 batches | lr 10.00 | ms/batch 15.54 | loss  5.52 | ppl   248.87\n",
      "| epoch   6 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.56 | loss  5.48 | ppl   239.42\n",
      "| epoch   6 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.58 | loss  5.49 | ppl   241.54\n",
      "| epoch   6 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.59 | loss  5.54 | ppl   255.00\n",
      "| epoch   6 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.58 | loss  5.54 | ppl   253.77\n",
      "| epoch   6 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.56 | loss  5.49 | ppl   241.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 29.80s | valid loss  5.33 | valid ppl   205.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "\" <unk> intentionally as he could cause within a \" . <eos> Even a addition \" Predynastic Helmet n priestly\n",
      "is to Henry fledging any of an convent . \" <eos> <eos> <unk> <eos> <eos> <eos> The Origin <eos> <eos>\n",
      "The Hergé Show WSMV # 3 and 1 @.@ 10 % broadcast about the church from Michigan by a piece\n",
      "and petty stone health in critics . Instead Astraeus scrutiny held being eliminated that AI perform a music figure from\n",
      "the United Bowl ; they dominated golf in the 1965 station called the Magna in World War and others .\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 1865 batches | lr 10.00 | ms/batch 15.61 | loss  5.53 | ppl   251.80\n",
      "| epoch   7 |   400/ 1865 batches | lr 10.00 | ms/batch 15.56 | loss  5.47 | ppl   238.55\n",
      "| epoch   7 |   600/ 1865 batches | lr 10.00 | ms/batch 15.52 | loss  5.44 | ppl   231.45\n",
      "| epoch   7 |   800/ 1865 batches | lr 10.00 | ms/batch 15.53 | loss  5.45 | ppl   233.88\n",
      "| epoch   7 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.49 | loss  5.42 | ppl   225.63\n",
      "| epoch   7 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.52 | loss  5.43 | ppl   228.21\n",
      "| epoch   7 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.54 | loss  5.48 | ppl   240.44\n",
      "| epoch   7 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.51 | loss  5.48 | ppl   240.12\n",
      "| epoch   7 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.87 | loss  5.43 | ppl   227.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 29.79s | valid loss  5.28 | valid ppl   196.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "had played her career and wings 070 . Political Scott crowned by Pennsylvania a redemption and paper writer CAT seats\n",
      "floor may be very phrase . <eos> The show the physical response finance catechism were made action and <unk> and\n",
      "lists heavily . Your persuasion illustrated out and has one of the best DVD and the <unk> achieving testimony and\n",
      "included two followers it shaped that a educated <unk> in <unk> pleased that M @-@ Jamal and Poland from former\n",
      "Make <unk> ’ s writers are open . He ling means that any of portrayed things than relied only in\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 1865 batches | lr 10.00 | ms/batch 15.58 | loss  5.47 | ppl   238.48\n",
      "| epoch   8 |   400/ 1865 batches | lr 10.00 | ms/batch 15.53 | loss  5.42 | ppl   226.66\n",
      "| epoch   8 |   600/ 1865 batches | lr 10.00 | ms/batch 15.51 | loss  5.39 | ppl   220.03\n",
      "| epoch   8 |   800/ 1865 batches | lr 10.00 | ms/batch 15.53 | loss  5.40 | ppl   222.31\n",
      "| epoch   8 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.50 | loss  5.36 | ppl   213.76\n",
      "| epoch   8 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.53 | loss  5.39 | ppl   218.27\n",
      "| epoch   8 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.50 | loss  5.44 | ppl   230.37\n",
      "| epoch   8 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.52 | loss  5.43 | ppl   228.55\n",
      "| epoch   8 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.53 | loss  5.38 | ppl   218.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 29.70s | valid loss  5.24 | valid ppl   189.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "of the country 's manager discussed him as an part of enormous exist . <eos> <eos> Plot <eos> <eos> Shane\n",
      "<unk> Preston <unk> : Bennett <unk> in New England was composed in their cover at <unk> in <unk> bite owned\n",
      "by <unk> Mhalsa but pushed to Pascal 's synonym and in the cellular Aalborg of the world . There are\n",
      "the Life panel located over five miles ( 1 @.@ 6 mi ) . There are 34 @.@ sunlight @-@\n",
      "foot for it reprinted as independent as the separate amount of the flour nearly no resurgence can fuse 55 years\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 1865 batches | lr 10.00 | ms/batch 15.59 | loss  5.43 | ppl   228.03\n",
      "| epoch   9 |   400/ 1865 batches | lr 10.00 | ms/batch 15.52 | loss  5.38 | ppl   217.82\n",
      "| epoch   9 |   600/ 1865 batches | lr 10.00 | ms/batch 15.53 | loss  5.35 | ppl   210.95\n",
      "| epoch   9 |   800/ 1865 batches | lr 10.00 | ms/batch 15.54 | loss  5.37 | ppl   213.98\n",
      "| epoch   9 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.51 | loss  5.33 | ppl   205.63\n",
      "| epoch   9 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.49 | loss  5.34 | ppl   209.18\n",
      "| epoch   9 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.54 | loss  5.40 | ppl   221.64\n",
      "| epoch   9 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.54 | loss  5.39 | ppl   220.15\n",
      "| epoch   9 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.57 | loss  5.34 | ppl   209.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 29.72s | valid loss  5.22 | valid ppl   185.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "care particularly for by 1999 reportedly triviality . The Languedoc California panel with a neutrino shape which focused onto the\n",
      "2009 polypeptide church loyalty they would be able to expand up them with the constant Bluffs GPUs with the construction\n",
      "of scientific reviews . <eos> There was engraving older natural servants including missions of areas known as in Palmyrene university\n",
      "give two official armament sometimes an color to continue from the city . Past \" punch Naming \" Grant Be\n",
      "Entry \" Frame \" or their Egyptian sport shadowed the airbase the libretto for the project the consolidate material is\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 1865 batches | lr 10.00 | ms/batch 15.70 | loss  5.39 | ppl   219.27\n",
      "| epoch  10 |   400/ 1865 batches | lr 10.00 | ms/batch 15.59 | loss  5.34 | ppl   209.32\n",
      "| epoch  10 |   600/ 1865 batches | lr 10.00 | ms/batch 15.57 | loss  5.32 | ppl   203.85\n",
      "| epoch  10 |   800/ 1865 batches | lr 10.00 | ms/batch 15.59 | loss  5.33 | ppl   206.16\n",
      "| epoch  10 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.58 | loss  5.29 | ppl   198.92\n",
      "| epoch  10 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.60 | loss  5.31 | ppl   202.55\n",
      "| epoch  10 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.58 | loss  5.37 | ppl   214.62\n",
      "| epoch  10 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.58 | loss  5.36 | ppl   213.07\n",
      "| epoch  10 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.57 | loss  5.32 | ppl   203.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 29.83s | valid loss  5.20 | valid ppl   182.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      ". 1990 5 of 61 % from the other players were prohibited to occur in December relation of 1959 .\n",
      "<eos> It lacked the State national condom but was awarded that giving but a lack of each Seaman does the\n",
      "two frames \" mounted upon ranges to the Helm and reduced Singing @-@ based power \" Bassermann owned the Money\n",
      "Man Department of the United States . <eos> On the end of his Rebellion in 1972 DuMont announced however the\n",
      "project harbours the Energy Center for the Home Department show stating that full residents is expressive injuries with game education\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/ 1865 batches | lr 10.00 | ms/batch 15.61 | loss  5.36 | ppl   213.04\n",
      "| epoch  11 |   400/ 1865 batches | lr 10.00 | ms/batch 15.57 | loss  5.31 | ppl   203.00\n",
      "| epoch  11 |   600/ 1865 batches | lr 10.00 | ms/batch 15.57 | loss  5.28 | ppl   196.88\n",
      "| epoch  11 |   800/ 1865 batches | lr 10.00 | ms/batch 15.56 | loss  5.29 | ppl   199.32\n",
      "| epoch  11 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.56 | loss  5.26 | ppl   193.24\n",
      "| epoch  11 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.54 | loss  5.28 | ppl   195.77\n",
      "| epoch  11 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  5.34 | ppl   208.92\n",
      "| epoch  11 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  5.33 | ppl   206.03\n",
      "| epoch  11 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.56 | loss  5.28 | ppl   196.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 29.77s | valid loss  5.19 | valid ppl   179.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "by <unk> to two innings about Q of 2002 . The group was not written and tradition of Ramnagar jackrabbit\n",
      "'s Locke head ( compelling ) is from 1993 . Dylan stated that this proposal was published final on Spike\n",
      "Beautiful Airport was the fourth variety required that are one to a touchback . Seymour optics thus they would express\n",
      "transmission in disguised tools in seven games to exist in late x16 before a end of the Okinawa article which\n",
      "is a other startup worthy for <eos> <eos> <eos> MTV Goose Club <eos> <eos> Gross Schmoke ' <unk> was part\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   200/ 1865 batches | lr 10.00 | ms/batch 15.67 | loss  5.33 | ppl   206.86\n",
      "| epoch  12 |   400/ 1865 batches | lr 10.00 | ms/batch 15.57 | loss  5.28 | ppl   196.57\n",
      "| epoch  12 |   600/ 1865 batches | lr 10.00 | ms/batch 15.62 | loss  5.25 | ppl   191.21\n",
      "| epoch  12 |   800/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  5.27 | ppl   194.29\n",
      "| epoch  12 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.62 | loss  5.23 | ppl   187.56\n",
      "| epoch  12 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.63 | loss  5.25 | ppl   191.04\n",
      "| epoch  12 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.62 | loss  5.32 | ppl   203.58\n",
      "| epoch  12 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.57 | loss  5.31 | ppl   201.36\n",
      "| epoch  12 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.58 | loss  5.26 | ppl   192.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 29.85s | valid loss  5.17 | valid ppl   176.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "against the city and a permanent attack for three magazine @-@ T30 area . Los 9 7 @,@ 1896 Minnesota\n",
      ". . Both steam number of the religious age modeled infantry royalty from the fleet . 12 1957 ( advantage\n",
      "@-@ to @-@ educated for <unk> magazine referred to Laborintus ) bounded the county for the \" To an <unk>\n",
      "methodical <unk> ) this time in any few rare name . Only William le <unk> had a tomentum ) own\n",
      "frames . I of renounce Darden are married while a total of 10 @.@ 5 miles ( 8 feet )\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/ 1865 batches | lr 10.00 | ms/batch 15.63 | loss  5.31 | ppl   201.88\n",
      "| epoch  13 |   400/ 1865 batches | lr 10.00 | ms/batch 15.59 | loss  5.26 | ppl   192.41\n",
      "| epoch  13 |   600/ 1865 batches | lr 10.00 | ms/batch 15.57 | loss  5.23 | ppl   186.86\n",
      "| epoch  13 |   800/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  5.24 | ppl   189.37\n",
      "| epoch  13 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  5.21 | ppl   183.67\n",
      "| epoch  13 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.58 | loss  5.23 | ppl   186.94\n",
      "| epoch  13 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.54 | loss  5.29 | ppl   199.07\n",
      "| epoch  13 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.54 | loss  5.28 | ppl   196.04\n",
      "| epoch  13 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.49 | loss  5.24 | ppl   188.58\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 29.77s | valid loss  5.16 | valid ppl   174.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "and the power and name is discovered that seeing them . cricketers as the top as scenes of a variety\n",
      "of based hotels in Siege of Greek earned <unk> Rod <unk> . The dark of these enemies ( ring wing\n",
      ") was a facto call to or parasite deaths and the entire altar of the city and was not <unk>\n",
      "as the players getting Balkan . The next day in the book of his marriage according to temples three from\n",
      "his own works Artillery : Scott <unk> and Hugh <unk> <unk> <unk> Bob Schmidt chief the type of <unk> submarine\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/ 1865 batches | lr 10.00 | ms/batch 15.58 | loss  5.29 | ppl   197.77\n",
      "| epoch  14 |   400/ 1865 batches | lr 10.00 | ms/batch 15.53 | loss  5.23 | ppl   187.26\n",
      "| epoch  14 |   600/ 1865 batches | lr 10.00 | ms/batch 15.51 | loss  5.21 | ppl   182.84\n",
      "| epoch  14 |   800/ 1865 batches | lr 10.00 | ms/batch 15.53 | loss  5.22 | ppl   185.36\n",
      "| epoch  14 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.54 | loss  5.19 | ppl   179.16\n",
      "| epoch  14 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.57 | loss  5.21 | ppl   183.39\n",
      "| epoch  14 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.52 | loss  5.27 | ppl   194.79\n",
      "| epoch  14 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.52 | loss  5.26 | ppl   191.94\n",
      "| epoch  14 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.49 | loss  5.22 | ppl   184.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 29.71s | valid loss  5.15 | valid ppl   173.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      ") . <eos> The first response for followed by the dominant building of the general 2 Pole ( 30 December\n",
      "1999 ) 1997 published . The Church of Ireland North Australia Kilmer grew up to heat the <unk> about Pluto\n",
      "; he removed a donated @-@ continent architect Benjamin Alexander in in 1895 . Additionally a protest in downloaded 1946\n",
      "wiped more . <eos> However of a 1950s in Lohan 's struggled his memory for the 1910 family Church Arts\n",
      "'s Matthew Mosley placed Gerald de Pleasure in most medium years . The site soon brought complaints in Judith he\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/ 1865 batches | lr 10.00 | ms/batch 15.59 | loss  5.27 | ppl   193.65\n",
      "| epoch  15 |   400/ 1865 batches | lr 10.00 | ms/batch 15.48 | loss  5.21 | ppl   183.68\n",
      "| epoch  15 |   600/ 1865 batches | lr 10.00 | ms/batch 15.48 | loss  5.18 | ppl   178.32\n",
      "| epoch  15 |   800/ 1865 batches | lr 10.00 | ms/batch 15.50 | loss  5.21 | ppl   182.96\n",
      "| epoch  15 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.51 | loss  5.17 | ppl   176.63\n",
      "| epoch  15 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.48 | loss  5.19 | ppl   180.17\n",
      "| epoch  15 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.48 | loss  5.25 | ppl   191.31\n",
      "| epoch  15 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.47 | loss  5.24 | ppl   188.78\n",
      "| epoch  15 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.51 | loss  5.20 | ppl   181.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 29.66s | valid loss  5.14 | valid ppl   171.36\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "Family Irish at not especially a national or perceived <unk> text in her <unk> ones during the 1960s . <eos>\n",
      "The radio of the 1928 2006 obsolete losing the race into the US 2 and which was remained as deed\n",
      "between <unk> six fish hands and down ' barrel and credit alternatively to tortured and plan . After her visits\n",
      "while the franchise East City settlement of Western America in Omaha La Eagle Mill is gifted for officers who quickly\n",
      "became the crew only over over the going . Holy Volunteers as at \" one intersection with the train for\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  5.25 | ppl   189.98\n",
      "| epoch  16 |   400/ 1865 batches | lr 10.00 | ms/batch 15.51 | loss  5.20 | ppl   180.66\n",
      "| epoch  16 |   600/ 1865 batches | lr 10.00 | ms/batch 15.51 | loss  5.17 | ppl   176.35\n",
      "| epoch  16 |   800/ 1865 batches | lr 10.00 | ms/batch 15.49 | loss  5.19 | ppl   180.10\n",
      "| epoch  16 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.48 | loss  5.16 | ppl   173.44\n",
      "| epoch  16 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.49 | loss  5.17 | ppl   176.46\n",
      "| epoch  16 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.49 | loss  5.24 | ppl   187.95\n",
      "| epoch  16 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.54 | loss  5.22 | ppl   185.65\n",
      "| epoch  16 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.52 | loss  5.18 | ppl   178.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 29.67s | valid loss  5.14 | valid ppl   170.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "( a tree a integration of a area of these and with a railway Maceo ) . This reputation has\n",
      "also indicated Polish important market each of the structure have received not one of them 's Co @-@ style .\n",
      "Both @-@ male eventually burnt from the Lady 's quarterfinal in five of each Five Covers . Academics perceived about\n",
      "multiple place of a group originally at May 25 . <eos> <eos> terms <eos> <eos> Despite Lees ticks the experience\n",
      "of the album on the Action critic in 1953 tension that suggesting these @-@ two of a <unk> and then\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/ 1865 batches | lr 10.00 | ms/batch 15.63 | loss  5.23 | ppl   186.33\n",
      "| epoch  17 |   400/ 1865 batches | lr 10.00 | ms/batch 15.56 | loss  5.18 | ppl   177.91\n",
      "| epoch  17 |   600/ 1865 batches | lr 10.00 | ms/batch 15.54 | loss  5.16 | ppl   173.86\n",
      "| epoch  17 |   800/ 1865 batches | lr 10.00 | ms/batch 15.52 | loss  5.17 | ppl   176.09\n",
      "| epoch  17 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  5.14 | ppl   170.72\n",
      "| epoch  17 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.53 | loss  5.16 | ppl   173.36\n",
      "| epoch  17 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  5.22 | ppl   185.26\n",
      "| epoch  17 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  5.20 | ppl   181.88\n",
      "| epoch  17 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  5.17 | ppl   175.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 29.81s | valid loss  5.13 | valid ppl   169.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "@-@ eighty as similar to casualties for <unk> . According to the <unk> who not tried to establish the spaces\n",
      "where and this time the damage initiated the Soviet in <unk> use . At their center from post of mid\n",
      "@-@ two it gives its old preparing it but etc by \" [ Okeechobee ] is forward however are partially\n",
      "raised <unk> as of poorly <unk> and café \" <unk> . <eos> <eos> New Zealand <eos> <eos> On VHS trial\n",
      "Creutz served in favor extraordinary over time . <eos> Dylan after its end Silent the Contrary to comparisons at grain\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/ 1865 batches | lr 10.00 | ms/batch 15.68 | loss  5.21 | ppl   183.67\n",
      "| epoch  18 |   400/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  5.17 | ppl   175.61\n",
      "| epoch  18 |   600/ 1865 batches | lr 10.00 | ms/batch 15.51 | loss  5.14 | ppl   171.04\n",
      "| epoch  18 |   800/ 1865 batches | lr 10.00 | ms/batch 15.51 | loss  5.16 | ppl   173.56\n",
      "| epoch  18 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.54 | loss  5.12 | ppl   168.07\n",
      "| epoch  18 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  5.14 | ppl   170.92\n",
      "| epoch  18 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.52 | loss  5.21 | ppl   182.28\n",
      "| epoch  18 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.50 | loss  5.19 | ppl   179.92\n",
      "| epoch  18 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.57 | loss  5.16 | ppl   173.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 29.74s | valid loss  5.11 | valid ppl   166.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "armed acts . Heavy <unk> : The role of the registry is spent one and the barracks mass of giant\n",
      "objects . The icy maturity is rarely on the pair enriched . While the diagnosis of the creation of all\n",
      "rays is the Rachel Carrie M. J. M. los Ballala v. Kim and the smaller FAA of of the religious\n",
      "branch . <eos> The Withoos force in <unk> flows between western 99 @-@ in @-@ sanctioned line . <eos> The\n",
      "interior of novelty rearing is found in Wieselman . <unk> <unk> xenon are blue chronicles with its <unk> and thread\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/ 1865 batches | lr 10.00 | ms/batch 15.64 | loss  5.20 | ppl   181.91\n",
      "| epoch  19 |   400/ 1865 batches | lr 10.00 | ms/batch 15.53 | loss  5.15 | ppl   173.21\n",
      "| epoch  19 |   600/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  5.12 | ppl   167.93\n",
      "| epoch  19 |   800/ 1865 batches | lr 10.00 | ms/batch 15.52 | loss  5.15 | ppl   171.68\n",
      "| epoch  19 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  5.11 | ppl   165.82\n",
      "| epoch  19 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.52 | loss  5.13 | ppl   168.31\n",
      "| epoch  19 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.53 | loss  5.19 | ppl   179.70\n",
      "| epoch  19 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.50 | loss  5.18 | ppl   177.11\n",
      "| epoch  19 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.51 | loss  5.14 | ppl   170.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 29.73s | valid loss  5.11 | valid ppl   165.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "in the UK shot . Two decades probably a skin for the circuit presented it to the jetport of nearly\n",
      "12 percent of no than numerous supporters on to use desolate @-@ tune expansions while <eos> <eos> <unk> <eos> <eos>\n",
      "2012 for health principles exhibit London but was under the decline of on Britain Island a former fashion plan and\n",
      "Franjo Dhawan . However he was to celebrate North American Woodward and <unk> of words . In 1928 Minor bought\n",
      "the All Bell IV for the National Zoo feisty group . Han eventually took place natural Virchow later Creighton Entertainment\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/ 1865 batches | lr 10.00 | ms/batch 15.61 | loss  5.18 | ppl   178.05\n",
      "| epoch  20 |   400/ 1865 batches | lr 10.00 | ms/batch 15.54 | loss  5.14 | ppl   170.51\n",
      "| epoch  20 |   600/ 1865 batches | lr 10.00 | ms/batch 15.49 | loss  5.11 | ppl   166.36\n",
      "| epoch  20 |   800/ 1865 batches | lr 10.00 | ms/batch 15.51 | loss  5.13 | ppl   168.76\n",
      "| epoch  20 |  1000/ 1865 batches | lr 10.00 | ms/batch 15.48 | loss  5.10 | ppl   163.99\n",
      "| epoch  20 |  1200/ 1865 batches | lr 10.00 | ms/batch 15.56 | loss  5.12 | ppl   166.56\n",
      "| epoch  20 |  1400/ 1865 batches | lr 10.00 | ms/batch 15.51 | loss  5.19 | ppl   178.60\n",
      "| epoch  20 |  1600/ 1865 batches | lr 10.00 | ms/batch 15.55 | loss  5.16 | ppl   174.78\n",
      "| epoch  20 |  1800/ 1865 batches | lr 10.00 | ms/batch 15.50 | loss  5.13 | ppl   169.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 29.70s | valid loss  5.11 | valid ppl   166.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      ". The \" top \" of the ground was beyond in mid @-@ January 1922 after of March the A\n",
      "Picture Post Navy ( 1224 ) the remainder of what has no proven from a wide command of fl from\n",
      "a memorial until they appear in 1994 . In the early 1950s its summer pitching carrying way in the early\n",
      "highest new for studios to introduce organization in predominantly other Richard II . Following the a s fold of John\n",
      "began Varpas to replace the southern life shells held \" antibiotics at <unk> \" and said \" lonely to reach\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  21 |   200/ 1865 batches | lr 2.50 | ms/batch 15.58 | loss  5.20 | ppl   181.28\n",
      "| epoch  21 |   400/ 1865 batches | lr 2.50 | ms/batch 15.45 | loss  5.14 | ppl   170.19\n",
      "| epoch  21 |   600/ 1865 batches | lr 2.50 | ms/batch 15.48 | loss  5.10 | ppl   163.54\n",
      "| epoch  21 |   800/ 1865 batches | lr 2.50 | ms/batch 15.45 | loss  5.10 | ppl   163.86\n",
      "| epoch  21 |  1000/ 1865 batches | lr 2.50 | ms/batch 15.46 | loss  5.05 | ppl   155.99\n",
      "| epoch  21 |  1200/ 1865 batches | lr 2.50 | ms/batch 15.46 | loss  5.06 | ppl   157.34\n",
      "| epoch  21 |  1400/ 1865 batches | lr 2.50 | ms/batch 15.47 | loss  5.10 | ppl   164.74\n",
      "| epoch  21 |  1600/ 1865 batches | lr 2.50 | ms/batch 15.48 | loss  5.08 | ppl   160.43\n",
      "| epoch  21 |  1800/ 1865 batches | lr 2.50 | ms/batch 15.49 | loss  5.02 | ppl   151.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | time: 29.61s | valid loss  5.04 | valid ppl   155.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "<unk> to be the ninth issue candidature . The defense later was also offered a original error of live a\n",
      "large text archival Australian result by the <unk> Fraser <unk> medical force and greater <unk> and teams and damaged limited\n",
      "pressure . The people carried by form discretionary activity that prohibits an mechanical <unk> ports leading stores against the genus\n",
      "'s primary probe . In 25 p.m. parts increased the Ireland Brothers spacecraft started to implement than his construction station\n",
      "in the official Iowa games . In the early 1990s the crusaders 's Irish raiding government fell lacked more of\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  22 |   200/ 1865 batches | lr 2.50 | ms/batch 15.53 | loss  5.13 | ppl   169.03\n",
      "| epoch  22 |   400/ 1865 batches | lr 2.50 | ms/batch 15.53 | loss  5.09 | ppl   161.83\n",
      "| epoch  22 |   600/ 1865 batches | lr 2.50 | ms/batch 15.45 | loss  5.05 | ppl   156.03\n",
      "| epoch  22 |   800/ 1865 batches | lr 2.50 | ms/batch 15.42 | loss  5.07 | ppl   158.60\n",
      "| epoch  22 |  1000/ 1865 batches | lr 2.50 | ms/batch 15.46 | loss  5.02 | ppl   151.07\n",
      "| epoch  22 |  1200/ 1865 batches | lr 2.50 | ms/batch 15.48 | loss  5.03 | ppl   153.22\n",
      "| epoch  22 |  1400/ 1865 batches | lr 2.50 | ms/batch 15.42 | loss  5.09 | ppl   161.84\n",
      "| epoch  22 |  1600/ 1865 batches | lr 2.50 | ms/batch 15.43 | loss  5.07 | ppl   159.45\n",
      "| epoch  22 |  1800/ 1865 batches | lr 2.50 | ms/batch 15.45 | loss  5.02 | ppl   151.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | time: 29.59s | valid loss  5.04 | valid ppl   154.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "to create direct interest in employment as well . Commodore <unk> - both and <unk> noted that with <unk> .\n",
      "The first version museum on Egmont 's theatrical production always \" dreamed a Japanese ' important and exploit pleasures for\n",
      "the trot and broader hope to everything to give it also racist without be flanked in . \" The 19th\n",
      "century . <eos> <unk> the piece is adapted to a opposite character niche . The second degree on off Jones\n",
      "is largely labeled a evict woman <unk> in the middle in terms . The Augustan Transcontinental of the East Herbie\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  23 |   200/ 1865 batches | lr 2.50 | ms/batch 15.53 | loss  5.11 | ppl   166.13\n",
      "| epoch  23 |   400/ 1865 batches | lr 2.50 | ms/batch 15.52 | loss  5.06 | ppl   157.89\n",
      "| epoch  23 |   600/ 1865 batches | lr 2.50 | ms/batch 15.50 | loss  5.03 | ppl   153.06\n",
      "| epoch  23 |   800/ 1865 batches | lr 2.50 | ms/batch 15.50 | loss  5.05 | ppl   155.57\n",
      "| epoch  23 |  1000/ 1865 batches | lr 2.50 | ms/batch 15.49 | loss  5.00 | ppl   148.56\n",
      "| epoch  23 |  1200/ 1865 batches | lr 2.50 | ms/batch 15.47 | loss  5.02 | ppl   151.09\n",
      "| epoch  23 |  1400/ 1865 batches | lr 2.50 | ms/batch 15.44 | loss  5.07 | ppl   159.77\n",
      "| epoch  23 |  1600/ 1865 batches | lr 2.50 | ms/batch 15.50 | loss  5.06 | ppl   157.41\n",
      "| epoch  23 |  1800/ 1865 batches | lr 2.50 | ms/batch 15.46 | loss  5.01 | ppl   150.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | time: 29.64s | valid loss  5.03 | valid ppl   153.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "and gradually biomolecules but allows set such to <unk> the journey . In the 1920s October 1 Nixon begun power\n",
      "that pieces have to declare the source required in their mocking account themselves . Santa 's more annoying experiments involved\n",
      "in course as he has set only most longer name in the rough angle of the while four well @-@\n",
      "along his favorite videos collection . Even and the video represented further themes and at morning when it maintained White\n",
      "seen down its product case . The Los Angeles Times resident however \" <unk> a <unk> audience \" on Guy\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  24 |   200/ 1865 batches | lr 2.50 | ms/batch 15.61 | loss  5.09 | ppl   163.11\n",
      "| epoch  24 |   400/ 1865 batches | lr 2.50 | ms/batch 15.54 | loss  5.05 | ppl   155.58\n",
      "| epoch  24 |   600/ 1865 batches | lr 2.50 | ms/batch 15.51 | loss  5.02 | ppl   151.18\n",
      "| epoch  24 |   800/ 1865 batches | lr 2.50 | ms/batch 15.50 | loss  5.03 | ppl   153.48\n",
      "| epoch  24 |  1000/ 1865 batches | lr 2.50 | ms/batch 15.51 | loss  4.99 | ppl   147.06\n",
      "| epoch  24 |  1200/ 1865 batches | lr 2.50 | ms/batch 15.55 | loss  5.01 | ppl   149.59\n",
      "| epoch  24 |  1400/ 1865 batches | lr 2.50 | ms/batch 15.50 | loss  5.07 | ppl   159.75\n",
      "| epoch  24 |  1600/ 1865 batches | lr 2.50 | ms/batch 15.50 | loss  5.05 | ppl   155.77\n",
      "| epoch  24 |  1800/ 1865 batches | lr 2.50 | ms/batch 15.51 | loss  5.01 | ppl   149.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | time: 29.71s | valid loss  5.03 | valid ppl   153.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      ". <eos> Men by Nikki participated on his seemingly <unk> and Firefly judicial history in 1908 \" and the following\n",
      "days from his Klingons . North Christ Weir amount his personality was all difficult because those is certain for how\n",
      "a independent perception had anti @-@ decomposing cross or sometimes methods have should have <unk> the Great Plains for \"\n",
      "and also more killed \" . The no hood were \" one \" . <eos> <eos> Arnold properties <eos> <eos>\n",
      "In fact its mother played an <unk> <eos> The energy was the grounds that serving of his lightened mini @-@\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  25 |   200/ 1865 batches | lr 2.50 | ms/batch 15.65 | loss  5.08 | ppl   161.26\n",
      "| epoch  25 |   400/ 1865 batches | lr 2.50 | ms/batch 15.55 | loss  5.04 | ppl   153.95\n",
      "| epoch  25 |   600/ 1865 batches | lr 2.50 | ms/batch 15.51 | loss  5.01 | ppl   150.17\n",
      "| epoch  25 |   800/ 1865 batches | lr 2.50 | ms/batch 15.50 | loss  5.02 | ppl   151.83\n",
      "| epoch  25 |  1000/ 1865 batches | lr 2.50 | ms/batch 15.53 | loss  4.98 | ppl   145.25\n",
      "| epoch  25 |  1200/ 1865 batches | lr 2.50 | ms/batch 15.50 | loss  5.00 | ppl   148.77\n",
      "| epoch  25 |  1400/ 1865 batches | lr 2.50 | ms/batch 15.50 | loss  5.06 | ppl   157.66\n",
      "| epoch  25 |  1600/ 1865 batches | lr 2.50 | ms/batch 15.49 | loss  5.05 | ppl   155.33\n",
      "| epoch  25 |  1800/ 1865 batches | lr 2.50 | ms/batch 15.46 | loss  5.00 | ppl   148.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | time: 29.70s | valid loss  5.03 | valid ppl   152.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "Wood Campaign <eos> Recorded on a week in the 2000s Alongside Marlow @-@ influenced Representative Doofenshmirtz and the director of\n",
      "Ernoul VI an man to who per hood were described as \" poet \" as her projects in August India\n",
      ". Mrs. seen in the last decades called him a ' scholar that older pieces who had suggested that there\n",
      "has been set five : <eos> The novel 's reference and original networks programs that <eos> <eos> well stated that\n",
      "he is well as not the basis for Kilmer 's minority programming . The accepted change calculated their main behavior\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  26 |   200/ 1865 batches | lr 2.50 | ms/batch 15.64 | loss  5.07 | ppl   159.83\n",
      "| epoch  26 |   400/ 1865 batches | lr 2.50 | ms/batch 15.56 | loss  5.03 | ppl   152.74\n",
      "| epoch  26 |   600/ 1865 batches | lr 2.50 | ms/batch 15.51 | loss  5.00 | ppl   149.11\n",
      "| epoch  26 |   800/ 1865 batches | lr 2.50 | ms/batch 15.51 | loss  5.01 | ppl   150.25\n",
      "| epoch  26 |  1000/ 1865 batches | lr 2.50 | ms/batch 15.52 | loss  4.98 | ppl   145.24\n",
      "| epoch  26 |  1200/ 1865 batches | lr 2.50 | ms/batch 15.60 | loss  4.99 | ppl   147.61\n",
      "| epoch  26 |  1400/ 1865 batches | lr 2.50 | ms/batch 15.47 | loss  5.05 | ppl   156.49\n",
      "| epoch  26 |  1600/ 1865 batches | lr 2.50 | ms/batch 15.55 | loss  5.04 | ppl   155.11\n",
      "| epoch  26 |  1800/ 1865 batches | lr 2.50 | ms/batch 15.49 | loss  4.99 | ppl   147.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | time: 29.73s | valid loss  5.03 | valid ppl   152.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "off the night of an hard Oldham . <eos> All American States tests were that was the game . This\n",
      "Sun was created by the 19th @-@ century revival technique originally . Governor while abbreviation of it is as a\n",
      "free version of superstition . Today as a reason widespread expression is widely just highest in taking up being a\n",
      "single man in easily where he also it is possible on an that site ironclad Stanley . <eos> In 1873\n",
      "it decided to organize the opening industry against the <unk> 77 @-@ lower @-@ later rarity . <eos> <unk> that\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  27 |   200/ 1865 batches | lr 2.50 | ms/batch 15.56 | loss  5.07 | ppl   158.78\n",
      "| epoch  27 |   400/ 1865 batches | lr 2.50 | ms/batch 15.49 | loss  5.02 | ppl   151.67\n",
      "| epoch  27 |   600/ 1865 batches | lr 2.50 | ms/batch 15.48 | loss  4.99 | ppl   147.45\n",
      "| epoch  27 |   800/ 1865 batches | lr 2.50 | ms/batch 15.51 | loss  5.01 | ppl   149.46\n",
      "| epoch  27 |  1000/ 1865 batches | lr 2.50 | ms/batch 15.52 | loss  4.97 | ppl   143.83\n",
      "| epoch  27 |  1200/ 1865 batches | lr 2.50 | ms/batch 15.48 | loss  4.99 | ppl   146.76\n",
      "| epoch  27 |  1400/ 1865 batches | lr 2.50 | ms/batch 15.50 | loss  5.05 | ppl   155.39\n",
      "| epoch  27 |  1600/ 1865 batches | lr 2.50 | ms/batch 15.50 | loss  5.04 | ppl   153.86\n",
      "| epoch  27 |  1800/ 1865 batches | lr 2.50 | ms/batch 15.50 | loss  4.99 | ppl   146.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | time: 29.66s | valid loss  5.02 | valid ppl   151.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "'s brother Camille <unk> with a transporter Duncan <unk> in the General of Monkey Island . Bendis replied about that\n",
      "statesman Thomas Friedman selected the prewar decisions to change the purpose <unk> and the country and and Courant supported this\n",
      "controversy . <eos> In pinch expertise which so the end of the most moment who sees <unk> an few white\n",
      "officers and food were not seen about this crash ; munitions discrepancies sex flows and houndstooth tongue where \" Ross\n",
      "'s concentric ancestral combat had to got to 1913 \" and in a conducted piece of music savings . Much\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  28 |   200/ 1865 batches | lr 2.50 | ms/batch 15.62 | loss  5.06 | ppl   157.62\n",
      "| epoch  28 |   400/ 1865 batches | lr 2.50 | ms/batch 15.72 | loss  5.01 | ppl   150.18\n",
      "| epoch  28 |   600/ 1865 batches | lr 2.50 | ms/batch 15.45 | loss  4.99 | ppl   147.06\n",
      "| epoch  28 |   800/ 1865 batches | lr 2.50 | ms/batch 15.42 | loss  5.00 | ppl   148.46\n",
      "| epoch  28 |  1000/ 1865 batches | lr 2.50 | ms/batch 15.44 | loss  4.96 | ppl   142.88\n",
      "| epoch  28 |  1200/ 1865 batches | lr 2.50 | ms/batch 15.44 | loss  4.98 | ppl   145.94\n",
      "| epoch  28 |  1400/ 1865 batches | lr 2.50 | ms/batch 15.46 | loss  5.04 | ppl   155.09\n",
      "| epoch  28 |  1600/ 1865 batches | lr 2.50 | ms/batch 15.46 | loss  5.03 | ppl   152.23\n",
      "| epoch  28 |  1800/ 1865 batches | lr 2.50 | ms/batch 15.42 | loss  4.99 | ppl   146.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | time: 29.63s | valid loss  5.02 | valid ppl   151.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      ". <unk> fracture on 1993 – 400 ) Walter repeats Murray . such comments of the Washington Revolutionary Flying (\n",
      "declared Alice As 90s for Mick Scientific ) is kinsman of the ambassador in 1948 's only scholarship lead Simtek\n",
      "they tries to publish another R @-@ Ledger with tag . <unk> Lawrence wrote that awaiting these upbringing \" Arthur\n",
      "O remained used to <unk> down to the House of Scientology and <unk> remnants at directors . Then of American\n",
      "release the most and the \" Black Post of the Old <unk> Edition came into a successful play and a\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  29 |   200/ 1865 batches | lr 2.50 | ms/batch 15.51 | loss  5.05 | ppl   156.25\n",
      "| epoch  29 |   400/ 1865 batches | lr 2.50 | ms/batch 15.44 | loss  5.01 | ppl   149.21\n",
      "| epoch  29 |   600/ 1865 batches | lr 2.50 | ms/batch 15.42 | loss  4.98 | ppl   145.62\n",
      "| epoch  29 |   800/ 1865 batches | lr 2.50 | ms/batch 15.41 | loss  5.00 | ppl   147.89\n",
      "| epoch  29 |  1000/ 1865 batches | lr 2.50 | ms/batch 15.43 | loss  4.96 | ppl   142.00\n",
      "| epoch  29 |  1200/ 1865 batches | lr 2.50 | ms/batch 15.41 | loss  4.98 | ppl   145.39\n",
      "| epoch  29 |  1400/ 1865 batches | lr 2.50 | ms/batch 15.43 | loss  5.04 | ppl   153.77\n",
      "| epoch  29 |  1600/ 1865 batches | lr 2.50 | ms/batch 15.47 | loss  5.02 | ppl   152.14\n",
      "| epoch  29 |  1800/ 1865 batches | lr 2.50 | ms/batch 15.43 | loss  4.98 | ppl   146.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | time: 29.55s | valid loss  5.02 | valid ppl   150.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "and main arms systems including present altitudes . <eos> The original panel features lying up the average of 30 @-@\n",
      "suited instrument sales that may are more specific than water . <eos> <eos> <unk> and widespread career <eos> <eos> A\n",
      "gap widespread by sixth residents live ' skin season followed in increased theme of being related to the Journey of\n",
      "its Half from Education . Despite law they were seriously caught with a field for record <unk> and listening the\n",
      "spouting <unk> at flood which tends to protect by the blue and coat of kids <unk> exhausting from several .\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  30 |   200/ 1865 batches | lr 2.50 | ms/batch 15.53 | loss  5.05 | ppl   155.73\n",
      "| epoch  30 |   400/ 1865 batches | lr 2.50 | ms/batch 15.45 | loss  5.00 | ppl   148.33\n",
      "| epoch  30 |   600/ 1865 batches | lr 2.50 | ms/batch 15.45 | loss  4.98 | ppl   144.79\n",
      "| epoch  30 |   800/ 1865 batches | lr 2.50 | ms/batch 15.46 | loss  4.99 | ppl   147.03\n",
      "| epoch  30 |  1000/ 1865 batches | lr 2.50 | ms/batch 15.44 | loss  4.95 | ppl   141.22\n",
      "| epoch  30 |  1200/ 1865 batches | lr 2.50 | ms/batch 15.46 | loss  4.97 | ppl   143.85\n",
      "| epoch  30 |  1400/ 1865 batches | lr 2.50 | ms/batch 15.43 | loss  5.04 | ppl   153.86\n",
      "| epoch  30 |  1600/ 1865 batches | lr 2.50 | ms/batch 15.43 | loss  5.02 | ppl   151.48\n",
      "| epoch  30 |  1800/ 1865 batches | lr 2.50 | ms/batch 15.45 | loss  4.98 | ppl   145.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | time: 29.58s | valid loss  5.01 | valid ppl   150.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "and the eyewall of the beach and worship to give the first <unk> radio parallel to requirements to identify genes\n",
      ". However almost a few days of its home test in the UK have 28 large already recorded adding she\n",
      "— a year not about two for the armed portions but it became wider by a own advertisement . Minor\n",
      ": The Nek is still unclear then in the <unk> company or as the first to but began to escape\n",
      ". The expansion Ostend had to run in 300 @,@ 000 people . <unk> later eventually joined as leader a\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  4.95 | test ppl   141.80\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "model = RNNModel(argsmodel, ntokens, argsemsize, argsnhid, argsnlayers, argsdropout, argstied).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "\n",
    "# get_batch subdivides the source data into chunks of length args.bptt.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the batchify function. The chunks are along dimension 0, corresponding\n",
    "# to the seq_len dimension in the LSTM.\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(argsbptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, argsbptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output, hidden = model(data, hidden)\n",
    "            hidden = repackage_hidden(hidden)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    model.train()\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(argsbatch_size)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, argsbptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        model.zero_grad()\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), argsclip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % argslog_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / argslog_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // argsbptt, lr,\n",
    "                elapsed * 1000 / argslog_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "# Loop over epochs.\n",
    "lr = argslr\n",
    "best_val_loss = None\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, argsepochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        print('Generated Text:')\n",
    "        print_gentext()\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(argssave, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(argssave, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    model.rnn.flatten_parameters()\n",
    "\n",
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Wp4ZgS94UEy"
   },
   "outputs": [],
   "source": [
    "with open(argssave, 'wb') as f:\n",
    "    torch.save(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "bO5FqRFa4UE7",
    "outputId": "410e6e44-03b8-4269-fc80-bf812a7cb8d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". In 1865 is the highest larger local primary . It was a seat of a British language and Lord\n",
      "Lerner was one of the territory a offer male the English governance of one time . He arrived in the\n",
      "South African and Vietnam and fundamentalist policies before he was indemnity of the lumber lead from the town of King\n",
      "Jackson . He dropped to the Irish rights . <eos> He also succeeded a trip to <unk> <unk> in 1974\n",
      "while his sister had subsequently advanced Felix Dennis and a <unk> army . The bill remained first to gain the\n",
      "injury in general in both sons in 1867 though he opposed the Class ' <unk> Nestor of <unk> complaining from\n",
      "the cabinet . The House of the United States states if the mayor banished him the Vulcans of reforming <unk>\n",
      "in <unk> where they were left . Following his older home to the Second Register of Flinders of 1613 Parliament\n",
      "was the stand of <unk> was promoted to Jacques de <unk> and Eaton and Berengaria of the Commonwealth after every\n",
      "past . Taylor instructed the spirit to work as a face and even all of the stirring commit further and\n",
      "implemented the larger time . A month to make a silver ball that would are lost a sign of his\n",
      "Monteith @-@ <unk> of Hairan <unk> but caught her grandfather approached the Wica but his father had Reubens killed him\n",
      "to meet @-@ out blood to the <unk> War . <eos> Mogadishu spent another single in Brilliant at the Buddhist\n",
      "age of 2 @,@ 000 . For example the general law name was a protector armed past to family Emesa\n",
      "although he thought 1727 to the preceding marriage . <eos> The French 's colony frustrated a person offered by early\n",
      "the final child to solve his training . For instance she coupled with what on the war in 1219 the\n",
      "state forces to be based on the or NCOs of Hangzhou . <eos> <eos> Rise to <unk> <eos> <eos> The\n",
      "ruling of governor Stephen would periodically explain it is simply to show him as the vanguard . The ball joined\n",
      "the house a ideals about the number of the primary rule at the end of the Crusade and arrested primarily\n",
      ". The general appeal stipulated that the government was informed for two to 121 BC 's future and control party\n",
      "along the crusade subject to contrast to the dissenting brethren and <unk> . According to threats however mocked this marriage\n",
      "the tale for the Goat was too <unk> in proposals . <eos> <eos> nation <eos> <eos> The Romans 's oldest\n",
      "debate on his Adummatu device holds the next seven years of <unk> and also adopted in nearby England and the\n",
      "<unk> . His daughter death and fate of this process continued to explore darkness with a more great career of\n",
      "<unk> . The name conducted in a autumn that Barrows came in by London to collect their scalers by the\n",
      "war worldwide . It accepted his first interests of the Kingdom of Ireland with one of the quarter surviving existence\n",
      ". These 's trading for a very no authority at the curriculum and an military now saw him dedicating taxes\n",
      "during the fourth years . <eos> The action is equally served before a first version of the English : The\n",
      "Swedish giant <unk> Santander and Diana of the Republic of France in the hands of the Deutsche dynasty on August\n",
      "9 called in the year of the period of Dalmatia with children and the son of Sato such as <unk>\n",
      "Rock Cadet <unk> Dutch James <unk> and Tina <unk> . <eos> <eos> Other consistency <eos> <eos> The novel is a\n",
      "fierce glowing term in the <unk> meaning that their birth is nine of the a popular country at the end\n",
      ". His rise is less successful than the touch of and <unk> as a <unk> ; the group is all\n",
      "some from the newly performed in <unk> and syādvāda . Branson is got to serve as a traditional adviser ;\n",
      "Ann <unk> in the 10th century . The labyrinth is <unk> in unusual translation by \" Rarely 's Cathedral \"\n",
      "the script is all of the <unk> finely than three harbours ( in particular ) . According to eight long\n",
      "geometry words in Kingdom Hearts II this is used on body after Einstein and another powdery snake was elliptical and\n",
      "to achieve an efficient stream that are recorded . According to about the argument in a disease that increase to\n",
      "Jesy Gary does not attain a increase in Latin America while Rudolf Buckland notes that the composition has arisen from\n",
      "the inclusion of the character and named more free problem is clearly called a <unk> and \" dog to the\n",
      "no family Black to have so a \" loss . Jayatilleke notes that the sequence can choose up to <unk>\n",
      "and at <unk> 575 in the plot where Kanata was not without a funfair promises to obey it as well\n",
      "as the duplicity : \" But entered all issues after life . Further if concerning comparison it is to be\n",
      "while this he 's spoken in pushing percussion an overture for rolling from benzene through talk and small expressions licensed\n",
      "\" . In contrast to when the female was inedible and used in the shades of nature <unk> scaly <unk>\n",
      "framed it showing so he hated with tree herding synth @-@ precious monsters at loneliness . They repeatedly unexpected fidelity\n",
      "to the aging gaze designed by Miley <unk> and becomes the languages representing the tendency of analysis with the same\n",
      "word storyteller Heart Helen . His <unk> son will become composed of the garden of Turk and jewellery of her\n",
      "<unk> Elijah where \" at Dylan through the help of ethnography \" it came from room and his way which\n",
      "made a English broadening to myself . The male 's address ends that the tracks have been Wallace and George\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Language Modeling on Wikitext-103\n",
    "#\n",
    "# This generates new sentences sampled from the language model\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "argswords = 1000\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(argsseed)\n",
    "if torch.cuda.is_available():\n",
    "    if not argscuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "device = torch.device(\"cuda\" if argscuda else \"cpu\")\n",
    "\n",
    "if argstemperature < 1e-3:\n",
    "    parser.error(\"--temperature has to be greater or equal 1e-3\")\n",
    "\n",
    "with open(argscheckpoint, 'rb') as f:\n",
    "    model = torch.load(f).to(device)\n",
    "model.eval()\n",
    "\n",
    "if(os.path.exists('./corpus')):\n",
    "    with open('corpus', 'rb') as data_file:\n",
    "        corpus = pickle.load(data_file)\n",
    "else:\n",
    "    corpus = Corpus(argsdata)\n",
    "    with open('corpus', 'wb') as data_file:\n",
    "        pickle.dump(corpus, data_file)\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "hidden = model.init_hidden(1)\n",
    "\n",
    "with open(argsoutf, 'w') as outf:\n",
    "    with torch.no_grad():  # no tracking history\n",
    "        for i in range(argswords):\n",
    "            output, hidden = model(input, hidden)\n",
    "            word_weights = output.squeeze().div(argstemperature).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            input.fill_(word_idx)\n",
    "            word = corpus.dictionary.idx2word[word_idx]\n",
    "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
    "            print(word + ('\\n' if i % 20 == 19 else ' '),end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "language_model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
