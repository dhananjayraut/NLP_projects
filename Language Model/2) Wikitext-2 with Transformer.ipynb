{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wGWK30EJRueT"
   },
   "source": [
    "[Data Set link](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "Fku86qKq4U-n",
    "outputId": "aae333b7-d288-422a-a77a-5bfe3ddd4d4a"
   },
   "outputs": [],
   "source": [
    "# !wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip\n",
    "# !unzip ./wikitext-103-v1.zip\n",
    "# !pip install torch==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "ianBd7ZoJ0xX",
    "outputId": "76a32c5e-ddf7-4edf-bdb9-2849d841e35f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-08-21 03:50:58--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\r\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.206.157\r\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.206.157|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 4475746 (4.3M) [application/zip]\r\n",
      "Saving to: ‘wikitext-2-v1.zip’\r\n",
      "\r\n",
      "wikitext-2-v1.zip   100%[===================>]   4.27M  9.50MB/s    in 0.4s    \r\n",
      "\r\n",
      "2019-08-21 03:50:58 (9.50 MB/s) - ‘wikitext-2-v1.zip’ saved [4475746/4475746]\r\n",
      "\r\n",
      "Archive:  ./wikitext-2-v1.zip\r\n",
      "   creating: wikitext-2/\r\n",
      "  inflating: wikitext-2/wiki.test.tokens  \r\n",
      "  inflating: wikitext-2/wiki.valid.tokens  \r\n",
      "  inflating: wikitext-2/wiki.train.tokens  \r\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n",
    "!unzip ./wikitext-2-v1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rk2JwHeeOPky"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ueCE8Rrg4UER"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from io import open\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "li = [',', '=']\n",
    "def valid(x):\n",
    "    if x in li:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(os.path.join(path, 'wiki.train.tokens'))\n",
    "        self.valid = self.tokenize(os.path.join(path, 'wiki.valid.tokens'))\n",
    "        self.test = self.tokenize(os.path.join(path, 'wiki.test.tokens'))\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \"\"\"Tokenizes a text file.\"\"\"\n",
    "        assert os.path.exists(path)\n",
    "        # Add words to the dictionary\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    if valid(word):\n",
    "                        self.dictionary.add_word(word)\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            idss = []\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                ids = []\n",
    "                for word in words:\n",
    "                    if valid(word):\n",
    "                        ids.append(self.dictionary.word2idx[word])\n",
    "                idss.append(torch.tensor(ids).type(torch.int64))\n",
    "            ids = torch.cat(idss)\n",
    "\n",
    "        return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLeqqjkz4UEd"
   },
   "outputs": [],
   "source": [
    "# Temporarily leave PositionalEncoding module here. Will be moved somewhere else.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent or transformer module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        try:\n",
    "            from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        except:\n",
    "            raise ImportError('TransformerEncoder module does not exist in PyTorch 1.1 or lower.')\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, has_mask=True):\n",
    "        if has_mask:\n",
    "            device = src.device\n",
    "            if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "                mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "                self.src_mask = mask\n",
    "        else:\n",
    "            self.src_mask = None\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return F.log_softmax(output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cGD7gniu5D3s"
   },
   "outputs": [],
   "source": [
    "argsdata = './wikitext-2' # or './wikitext-103'\n",
    "argsbatch_size = 40\n",
    "argsemsize=200\n",
    "argsnhead=2\n",
    "argsnhid=200\n",
    "argsnlayers=2\n",
    "argsdropout=0.2\n",
    "argslog_interval=200\n",
    "argsclip=0.5\n",
    "argsseed=42\n",
    "argsbptt=35\n",
    "argscuda=True\n",
    "argslr=5\n",
    "argsepochs=20\n",
    "argstemperature = 1.0\n",
    "argssave='./model.pt'\n",
    "argscheckpoint = './model.pt'\n",
    "argsoutf='generated.txt'\n",
    "argswords=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "jL0kAcJg6_bl",
    "outputId": "57231951-de39-412b-989c-e446f79a9bff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.74 s, sys: 896 ms, total: 6.64 s\n",
      "Wall time: 8.72 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(argsseed)\n",
    "if torch.cuda.is_available():\n",
    "    if not argscuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "device = torch.device(\"cuda\" if argscuda else \"cpu\")\n",
    "###############################################################################\n",
    "# Load data\n",
    "###############################################################################\n",
    "\n",
    "if(os.path.exists('./corpus')):\n",
    "    with open('corpus', 'rb') as data_file:\n",
    "        corpus = pickle.load(data_file)\n",
    "else:\n",
    "    corpus = Corpus(argsdata)\n",
    "    with open('corpus', 'wb') as data_file:\n",
    "        pickle.dump(corpus, data_file)\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "# Starting from sequential data, batchify arranges the dataset into columns.\n",
    "# For instance, with the alphabet as the sequence and batch size 4, we'd get\n",
    "# ┌ a g m s ┐\n",
    "# │ b h n t │\n",
    "# │ c i o u │\n",
    "# │ d j p v │\n",
    "# │ e k q w │\n",
    "# └ f l r x ┘.\n",
    "# These columns are treated as independent by the model, which means that the\n",
    "# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n",
    "# batch processing.\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "eval_batch_size = 100\n",
    "train_data = batchify(corpus.train, argsbatch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-6miY5HTI9oV"
   },
   "outputs": [],
   "source": [
    "def print_gentext():\n",
    "    \"\"\"Generate some example text form model \"\"\"\n",
    "    input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "    with torch.no_grad():  # no tracking history\n",
    "        for i in range(argswords):\n",
    "            output = model(input, False)\n",
    "            word_weights = output[-1].squeeze().div(argstemperature).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
    "            input = torch.cat([input, word_tensor], 0)\n",
    "            word = corpus.dictionary.idx2word[word_idx]\n",
    "            print(word + ('\\n' if i % 20 == 19 else ' '),end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2-TAv-w14UEn",
    "outputId": "29d2b79a-ec43-446b-bac1-52629e3577ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 1399 batches | lr 5.00 | ms/batch 26.84 | loss  8.54 | ppl  5113.04\n",
      "| epoch   1 |   400/ 1399 batches | lr 5.00 | ms/batch 22.97 | loss  7.25 | ppl  1401.47\n",
      "| epoch   1 |   600/ 1399 batches | lr 5.00 | ms/batch 23.10 | loss  6.81 | ppl   910.84\n",
      "| epoch   1 |   800/ 1399 batches | lr 5.00 | ms/batch 22.97 | loss  6.59 | ppl   726.74\n",
      "| epoch   1 |  1000/ 1399 batches | lr 5.00 | ms/batch 22.90 | loss  6.49 | ppl   655.78\n",
      "| epoch   1 |  1200/ 1399 batches | lr 5.00 | ms/batch 22.97 | loss  6.39 | ppl   595.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 33.94s | valid loss  6.08 | valid ppl   435.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "ranging in 1996 before fifteenth close to establish Alfa of an cousin of in the symbolized then announced a picture\n",
      "of women in South Africa for the transcriptional 2 metres ( extension from a stegosaurid that prefix ft ) ;\n",
      "their groups of 8 m ) . establishment of reggaeton ' true route was accompanied two Council Nazaire of Chelsea\n",
      "'s state of Juan counties from amphibians affected by critics were introduced in Florida complete miners matter was mortally appeals\n",
      "( 1975 . Lewiston Operationally farther Dahlgren recorded at first sources with her snapping with nifurtimox for Lumber grandfather extended\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 1399 batches | lr 5.00 | ms/batch 23.14 | loss  6.27 | ppl   526.81\n",
      "| epoch   2 |   400/ 1399 batches | lr 5.00 | ms/batch 23.18 | loss  6.17 | ppl   477.69\n",
      "| epoch   2 |   600/ 1399 batches | lr 5.00 | ms/batch 23.22 | loss  6.03 | ppl   415.03\n",
      "| epoch   2 |   800/ 1399 batches | lr 5.00 | ms/batch 23.14 | loss  5.99 | ppl   398.04\n",
      "| epoch   2 |  1000/ 1399 batches | lr 5.00 | ms/batch 23.14 | loss  5.96 | ppl   385.77\n",
      "| epoch   2 |  1200/ 1399 batches | lr 5.00 | ms/batch 23.22 | loss  5.94 | ppl   378.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 33.43s | valid loss  5.83 | valid ppl   340.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "<unk> of a unique of the commission of Walpole and Denardo it does not occasionally when the midfielder ( married\n",
      "to a woman and music Award [ Gray were given for Beaumont takes place in my life experiences with alternate\n",
      "and led the personal powers on and establishes for repaired an event she appear in Tales in contrast the shape\n",
      "and really none in any ghost @-@ <unk> in his absent \" ) but she was \" community of point\n",
      ". Although the paradoxical enough enjoy any of permission / and reasoned that he thought he intended for the Arsenal\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 1399 batches | lr 5.00 | ms/batch 23.28 | loss  5.91 | ppl   367.38\n",
      "| epoch   3 |   400/ 1399 batches | lr 5.00 | ms/batch 23.36 | loss  5.85 | ppl   346.88\n",
      "| epoch   3 |   600/ 1399 batches | lr 5.00 | ms/batch 23.21 | loss  5.71 | ppl   301.07\n",
      "| epoch   3 |   800/ 1399 batches | lr 5.00 | ms/batch 23.15 | loss  5.70 | ppl   297.73\n",
      "| epoch   3 |  1000/ 1399 batches | lr 5.00 | ms/batch 23.19 | loss  5.68 | ppl   294.37\n",
      "| epoch   3 |  1200/ 1399 batches | lr 5.00 | ms/batch 23.19 | loss  5.68 | ppl   294.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 33.52s | valid loss  5.73 | valid ppl   308.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "has been completely \" extremely \" and \" by lumbering \" ( \" ) . Within a medium is written\n",
      "by prisoner processes elaborate <unk> fiction Comparisons material 'Or and interwar ideas of cadmium <unk> of young Christian work with\n",
      "selections of Dahlen would continue to his <unk> \" . This lens architectural desire of seeing it artwork in North\n",
      "America made from her live within the show begins frog Terrorist or sometimes small Düsseldorf safety of the ' friend\n",
      "of similar to Christiana or gills in <unk> \" Can 't perfect grows around the end of spores are seen\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 1399 batches | lr 5.00 | ms/batch 23.26 | loss  5.68 | ppl   292.45\n",
      "| epoch   4 |   400/ 1399 batches | lr 5.00 | ms/batch 23.23 | loss  5.64 | ppl   281.56\n",
      "| epoch   4 |   600/ 1399 batches | lr 5.00 | ms/batch 23.36 | loss  5.50 | ppl   245.87\n",
      "| epoch   4 |   800/ 1399 batches | lr 5.00 | ms/batch 23.72 | loss  5.51 | ppl   246.28\n",
      "| epoch   4 |  1000/ 1399 batches | lr 5.00 | ms/batch 23.27 | loss  5.50 | ppl   244.34\n",
      "| epoch   4 |  1200/ 1399 batches | lr 5.00 | ms/batch 23.23 | loss  5.51 | ppl   246.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 33.66s | valid loss  5.69 | valid ppl   297.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      ". <eos> In 2016 relative to <unk> in injuries are of 8 @-@ yard line with a 3 and 5\n",
      "@-@ yard line with the final Syracuse of 13 . During this night between thumb and <unk> among seven @-@\n",
      "yard line had three O 'Malley held ... by <unk> GBA with five @-@ yard line Julian first down every\n",
      "antics their treble Trio @-@ yard fourth down when he threw Éditions ser pass up Millennium Stadium in the 69\n",
      "playoff pass they co @-@ yard and he earned run said Cairns which he then rushed for much of The\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 1399 batches | lr 5.00 | ms/batch 23.36 | loss  5.51 | ppl   247.44\n",
      "| epoch   5 |   400/ 1399 batches | lr 5.00 | ms/batch 23.26 | loss  5.48 | ppl   240.99\n",
      "| epoch   5 |   600/ 1399 batches | lr 5.00 | ms/batch 23.17 | loss  5.34 | ppl   209.54\n",
      "| epoch   5 |   800/ 1399 batches | lr 5.00 | ms/batch 23.27 | loss  5.36 | ppl   212.70\n",
      "| epoch   5 |  1000/ 1399 batches | lr 5.00 | ms/batch 23.38 | loss  5.35 | ppl   211.44\n",
      "| epoch   5 |  1200/ 1399 batches | lr 5.00 | ms/batch 23.24 | loss  5.37 | ppl   215.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 33.56s | valid loss  5.68 | valid ppl   292.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "of an <unk> . lock Dimitri fragilis usually used as an to or sweating \" and did not split into\n",
      "a wide approval at the end of early in Mosley and was built in 1999 as a specially adopted as\n",
      "a war . <eos> <unk> Brothers Records . He offered to be Viscount 's counterpart at the film titled White\n",
      "Kantara by a main entrance toward the end of America at <unk> Johnston cruise entered the writers that year in\n",
      "the building that year in February 1950 . Watts . <eos> Towards the audience and owner George Lorenzo Uttar Pradesh\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 1399 batches | lr 5.00 | ms/batch 23.26 | loss  5.38 | ppl   216.94\n",
      "| epoch   6 |   400/ 1399 batches | lr 5.00 | ms/batch 23.31 | loss  5.36 | ppl   212.56\n",
      "| epoch   6 |   600/ 1399 batches | lr 5.00 | ms/batch 23.22 | loss  5.22 | ppl   185.23\n",
      "| epoch   6 |   800/ 1399 batches | lr 5.00 | ms/batch 23.29 | loss  5.24 | ppl   189.15\n",
      "| epoch   6 |  1000/ 1399 batches | lr 5.00 | ms/batch 23.34 | loss  5.24 | ppl   188.26\n",
      "| epoch   6 |  1200/ 1399 batches | lr 5.00 | ms/batch 23.22 | loss  5.26 | ppl   192.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 33.56s | valid loss  5.67 | valid ppl   290.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "and all of its business governments Michael <unk> at Romani which were stopped to convert to hear North Korean troops\n",
      "four campaigns in England . Six Nations units into <unk> Mortal Kombat 2 Independent King of a Lincoln at a\n",
      "4 June 1943 at <unk> a half at the fourth largest part of antibiotic further starred in later in 1969\n",
      "Saladin of a level <unk> 2 July 18 2008 against the Action @-@ yard internal Wells for Mark Gallagher and\n",
      "a major convention Nixon as well of Fifth House of a feud with the ball away from the 80 days\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 1399 batches | lr 5.00 | ms/batch 23.48 | loss  5.27 | ppl   195.02\n",
      "| epoch   7 |   400/ 1399 batches | lr 5.00 | ms/batch 23.47 | loss  5.25 | ppl   191.21\n",
      "| epoch   7 |   600/ 1399 batches | lr 5.00 | ms/batch 23.35 | loss  5.12 | ppl   167.48\n",
      "| epoch   7 |   800/ 1399 batches | lr 5.00 | ms/batch 23.24 | loss  5.14 | ppl   171.47\n",
      "| epoch   7 |  1000/ 1399 batches | lr 5.00 | ms/batch 23.28 | loss  5.14 | ppl   171.43\n",
      "| epoch   7 |  1200/ 1399 batches | lr 5.00 | ms/batch 23.18 | loss  5.17 | ppl   175.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 33.63s | valid loss  5.66 | valid ppl   286.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "his younger siblings with T. Kody 348 and Langston Hughes Stephen Rose who can give him the Jima by coup\n",
      "d <unk> between Kody and Robyn she were mentioned in favor of her daughter on 19 Busch 's book <unk>\n",
      ". <eos> <eos> Charts in his own wives and Claire <unk> . Busch with the Tegetthoff nature from how Applewhite\n",
      ": Jagannadh <unk> with the books and David communicate with the return them seen as someone who had written new\n",
      "regime 's daughter Banai had read rape victim of archaeological evidence that the second wife she recorded threats from Ludlow\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 1399 batches | lr 5.00 | ms/batch 23.46 | loss  5.18 | ppl   178.27\n",
      "| epoch   8 |   400/ 1399 batches | lr 5.00 | ms/batch 23.24 | loss  5.17 | ppl   175.05\n",
      "| epoch   8 |   600/ 1399 batches | lr 5.00 | ms/batch 23.22 | loss  5.04 | ppl   154.18\n",
      "| epoch   8 |   800/ 1399 batches | lr 5.00 | ms/batch 23.27 | loss  5.06 | ppl   157.40\n",
      "| epoch   8 |  1000/ 1399 batches | lr 5.00 | ms/batch 23.17 | loss  5.06 | ppl   158.07\n",
      "| epoch   8 |  1200/ 1399 batches | lr 5.00 | ms/batch 23.23 | loss  5.09 | ppl   162.52\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 33.55s | valid loss  5.68 | valid ppl   292.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "from both Zrínyi and she refused to select <unk> a state as a radio traffic volume source at Works for\n",
      "considerable point . With more effective it was voted to induce the Producer Nick Saban due to a complete until\n",
      "10 November 30 and second half of the middle of Virginia Tech received in membership of the third quarter @-@\n",
      "distance north along the game short @-@ yard line with penalty . He concluded that time . The day over\n",
      "5 is growing improvement over 965 . The next Saturday Night Live performances in the mixed and an advocate of\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 1399 batches | lr 1.25 | ms/batch 23.40 | loss  4.98 | ppl   145.65\n",
      "| epoch   9 |   400/ 1399 batches | lr 1.25 | ms/batch 23.26 | loss  4.91 | ppl   136.21\n",
      "| epoch   9 |   600/ 1399 batches | lr 1.25 | ms/batch 23.29 | loss  4.76 | ppl   116.41\n",
      "| epoch   9 |   800/ 1399 batches | lr 1.25 | ms/batch 23.25 | loss  4.75 | ppl   115.54\n",
      "| epoch   9 |  1000/ 1399 batches | lr 1.25 | ms/batch 23.27 | loss  4.73 | ppl   112.86\n",
      "| epoch   9 |  1200/ 1399 batches | lr 1.25 | ms/batch 23.21 | loss  4.72 | ppl   112.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 33.57s | valid loss  5.60 | valid ppl   270.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "Ingram fumbled the ball to punt an unofficial Panthers touchdown reception for a March 3 – 4 – 2 @-@\n",
      "yard dash in the first down after Tech . After the lead Virginia Tech returned before the Hokies would run\n",
      "fire on offense after NC State agreed from the Hokies set to field goal was the lead to the field\n",
      "goal went 2 @-@ yard pass on the kick field . NC State offense began . Water ground assault on\n",
      "the Hokies took a child . On the Peach Bowl Alabama 's offense began a 30 seconds remaining teams however\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 1399 batches | lr 1.25 | ms/batch 23.36 | loss  4.84 | ppl   126.62\n",
      "| epoch  10 |   400/ 1399 batches | lr 1.25 | ms/batch 23.43 | loss  4.81 | ppl   122.31\n",
      "| epoch  10 |   600/ 1399 batches | lr 1.25 | ms/batch 23.23 | loss  4.67 | ppl   106.41\n",
      "| epoch  10 |   800/ 1399 batches | lr 1.25 | ms/batch 23.25 | loss  4.67 | ppl   107.08\n",
      "| epoch  10 |  1000/ 1399 batches | lr 1.25 | ms/batch 23.22 | loss  4.66 | ppl   105.51\n",
      "| epoch  10 |  1200/ 1399 batches | lr 1.25 | ms/batch 23.31 | loss  4.67 | ppl   106.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 33.60s | valid loss  5.60 | valid ppl   270.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      ". <eos> Angelou called the only one of literature : Hornung 's book Hornung 's character \" in gold dollar\n",
      "( 2010 ) . On December 7 1995 she compares his suggestion to his parents to run analysis the book\n",
      "in Formula One Ken Cambridge led to Liz Lemon Jelly published a rabbi because it has been made her thoughts\n",
      "fifth son of the Pharaoh <unk> Bradbury 's radical <unk> . She concludes that he joined the second on Wikipedia\n",
      "described a book . Harrison for a bottle said to be made the role along with a \" I Need\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   200/ 1399 batches | lr 0.31 | ms/batch 23.40 | loss  4.82 | ppl   124.10\n",
      "| epoch  11 |   400/ 1399 batches | lr 0.31 | ms/batch 23.21 | loss  4.78 | ppl   119.10\n",
      "| epoch  11 |   600/ 1399 batches | lr 0.31 | ms/batch 23.27 | loss  4.63 | ppl   102.37\n",
      "| epoch  11 |   800/ 1399 batches | lr 0.31 | ms/batch 23.23 | loss  4.63 | ppl   102.45\n",
      "| epoch  11 |  1000/ 1399 batches | lr 0.31 | ms/batch 23.36 | loss  4.60 | ppl    99.94\n",
      "| epoch  11 |  1200/ 1399 batches | lr 0.31 | ms/batch 23.31 | loss  4.60 | ppl    99.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 33.61s | valid loss  5.54 | valid ppl   255.68\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "DNA <unk> fluid thus removing metabolism . <eos> Development <eos> <eos> <eos> <eos> <eos> <unk> <eos> <eos> In a younger\n",
      "brother of New York Times Square Enix ( 1816 ) <eos> It was <unk> ( A paper published in 1963\n",
      ") <eos> <unk> sat on the daughter of Feature - ( 1976 ) <eos> <eos> Mkhedruli of The island ;\n",
      "warm by other composers Briggs ' n @-@ sixth century . Laborintus II * Be Stupid ( 1995 ) and\n",
      "a stone . She later written in North Carolina 's <unk> <unk> <eos> <eos> <eos> <eos> In that year and\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   200/ 1399 batches | lr 0.31 | ms/batch 23.31 | loss  4.77 | ppl   118.24\n",
      "| epoch  12 |   400/ 1399 batches | lr 0.31 | ms/batch 23.29 | loss  4.73 | ppl   113.52\n",
      "| epoch  12 |   600/ 1399 batches | lr 0.31 | ms/batch 23.23 | loss  4.59 | ppl    98.50\n",
      "| epoch  12 |   800/ 1399 batches | lr 0.31 | ms/batch 23.25 | loss  4.60 | ppl    99.32\n",
      "| epoch  12 |  1000/ 1399 batches | lr 0.31 | ms/batch 23.32 | loss  4.58 | ppl    97.48\n",
      "| epoch  12 |  1200/ 1399 batches | lr 0.31 | ms/batch 23.25 | loss  4.58 | ppl    97.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 33.57s | valid loss  5.54 | valid ppl   254.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "and cash out $ 350 million . He scored two goals over the Tigers from the playoffs in two days\n",
      "after World Cup . The Alabama named George Reid then hosted three @-@ game 's first @-@ CD and seven\n",
      "seasons ; however moved in 2014 – 10 – 13 of December 2004 NFL Draft Scout record for his new\n",
      "NHL All @-@ minute and The Bulls finished total @-@ oriented game on 17 @-@ levels in high school play\n",
      "@-@ 34 on 27 games to win seven times that first title track on 22 2010 while four @-@ CD\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   200/ 1399 batches | lr 0.31 | ms/batch 23.35 | loss  4.75 | ppl   115.18\n",
      "| epoch  13 |   400/ 1399 batches | lr 0.31 | ms/batch 23.28 | loss  4.71 | ppl   111.12\n",
      "| epoch  13 |   600/ 1399 batches | lr 0.31 | ms/batch 23.98 | loss  4.57 | ppl    96.45\n",
      "| epoch  13 |   800/ 1399 batches | lr 0.31 | ms/batch 23.30 | loss  4.58 | ppl    97.52\n",
      "| epoch  13 |  1000/ 1399 batches | lr 0.31 | ms/batch 23.34 | loss  4.56 | ppl    96.06\n",
      "| epoch  13 |  1200/ 1399 batches | lr 0.31 | ms/batch 23.30 | loss  4.57 | ppl    96.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 33.75s | valid loss  5.54 | valid ppl   255.48\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "on December 7 2009 the show elegiac publication originated from wild nature of God it . <eos> <eos> \" The\n",
      "first television critic suggested that the conformity should end worker hired a psalter doubts about its narrative greater grain gentleman\n",
      "that it paid an influence did not clear how Nesbitt was merely quit this terrible reason why he feels like\n",
      "they finally turn thriller \" ; he found to do not pass into heterogeneous character or finding a \" and\n",
      "said that of the Predatory Wasp hitting interesting \" . <eos> When Hornung 's melody about him a reference to\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   200/ 1399 batches | lr 0.08 | ms/batch 23.40 | loss  4.77 | ppl   118.24\n",
      "| epoch  14 |   400/ 1399 batches | lr 0.08 | ms/batch 23.27 | loss  4.74 | ppl   114.44\n",
      "| epoch  14 |   600/ 1399 batches | lr 0.08 | ms/batch 23.24 | loss  4.60 | ppl    99.22\n",
      "| epoch  14 |   800/ 1399 batches | lr 0.08 | ms/batch 23.49 | loss  4.60 | ppl    99.52\n",
      "| epoch  14 |  1000/ 1399 batches | lr 0.08 | ms/batch 23.38 | loss  4.57 | ppl    96.76\n",
      "| epoch  14 |  1200/ 1399 batches | lr 0.08 | ms/batch 23.18 | loss  4.57 | ppl    96.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 33.61s | valid loss  5.53 | valid ppl   252.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "of genes that are <unk> or is usually owned by eating europium . <eos> In <unk> <unk> poisoning ( Guatemala\n",
      ") are visible in English ornithologist John <unk> ( <unk> ) <unk> ( <unk> ) . <unk> Morrison ) is\n",
      "<unk> @-@ <unk> <unk> ) are unnumbered <unk> @-@ <unk> spp <unk> or simply the stalk ) . The edibility\n",
      "wing 's crust @-@ infrared beta decay of <unk> ) abeam ) have the game also <unk> <unk> roosting and\n",
      "<unk> ) it was present that refers to being supported by an anagram is a box @-@ linear DNA replication\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   200/ 1399 batches | lr 0.08 | ms/batch 23.32 | loss  4.76 | ppl   116.26\n",
      "| epoch  15 |   400/ 1399 batches | lr 0.08 | ms/batch 23.27 | loss  4.72 | ppl   111.65\n",
      "| epoch  15 |   600/ 1399 batches | lr 0.08 | ms/batch 23.14 | loss  4.58 | ppl    97.71\n",
      "| epoch  15 |   800/ 1399 batches | lr 0.08 | ms/batch 23.19 | loss  4.59 | ppl    98.26\n",
      "| epoch  15 |  1000/ 1399 batches | lr 0.08 | ms/batch 23.24 | loss  4.57 | ppl    96.29\n",
      "| epoch  15 |  1200/ 1399 batches | lr 0.08 | ms/batch 23.25 | loss  4.57 | ppl    96.44\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 33.49s | valid loss  5.52 | valid ppl   250.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      ". Following the storm neared highway passes through downtown Auburn was truncated east – 220 kilometres ( Dot that passes\n",
      "or six miles ( 89 km ) north of Veracruz along the eastern Utah Mississippi River which further east of\n",
      "the west of New Haven as Pennsylvania Columbia <unk> . The hurricane . <eos> <eos> Sully made landfall Lake Champlain\n",
      "Bridge \" better paved 1800 UTC megalithic river tributaries and <unk> River Avenue split into a tropical storm intensity of\n",
      "Stalingrad road later became the eastern Washington D.C. Leptoceratopsidae thereafter that designation of Saginaw Texas textiles crossed the northeast coast\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   200/ 1399 batches | lr 0.08 | ms/batch 23.28 | loss  4.74 | ppl   114.93\n",
      "| epoch  16 |   400/ 1399 batches | lr 0.08 | ms/batch 23.20 | loss  4.71 | ppl   110.66\n",
      "| epoch  16 |   600/ 1399 batches | lr 0.08 | ms/batch 23.15 | loss  4.57 | ppl    96.66\n",
      "| epoch  16 |   800/ 1399 batches | lr 0.08 | ms/batch 23.13 | loss  4.58 | ppl    97.52\n",
      "| epoch  16 |  1000/ 1399 batches | lr 0.08 | ms/batch 23.18 | loss  4.56 | ppl    95.56\n",
      "| epoch  16 |  1200/ 1399 batches | lr 0.08 | ms/batch 23.07 | loss  4.57 | ppl    96.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 33.40s | valid loss  5.51 | valid ppl   246.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "to Baltasar de Ibarra and often and over half the Irish Sea about the public talk about 1 : 16\n",
      "minutes when he <unk> in 2009 the Knights Hospitaller castles Campaign . After the organisation was attended the king McGrath\n",
      "had intended to Huainan works the fort was annexed into the NZ $ 250 @,@ 000 Colonels the family of\n",
      "Anglesey says that should be a canteen and demolished the king of <unk> on 9 @,@ 000 ; in 1796\n",
      "work was held up in 1898 . In 1929 after his works take the term \" New York 's reign\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   200/ 1399 batches | lr 0.08 | ms/batch 23.18 | loss  4.74 | ppl   114.76\n",
      "| epoch  17 |   400/ 1399 batches | lr 0.08 | ms/batch 23.15 | loss  4.70 | ppl   109.95\n",
      "| epoch  17 |   600/ 1399 batches | lr 0.08 | ms/batch 23.19 | loss  4.57 | ppl    96.17\n",
      "| epoch  17 |   800/ 1399 batches | lr 0.08 | ms/batch 23.08 | loss  4.57 | ppl    96.90\n",
      "| epoch  17 |  1000/ 1399 batches | lr 0.08 | ms/batch 23.12 | loss  4.56 | ppl    95.15\n",
      "| epoch  17 |  1200/ 1399 batches | lr 0.08 | ms/batch 23.02 | loss  4.56 | ppl    95.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 33.34s | valid loss  5.51 | valid ppl   246.49\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "\" blue \" . <eos> Due to the video for Olivier received the 2011 documentary <eos> Beyoncé Knowles <eos> <eos>\n",
      "<eos> \" The <unk> wrote \" Vitamin D \" garnered positive acclaim \" Of Will Be Stupid nominated for seven\n",
      "games for the song \" Outstanding Lead vocals generally selected the same title track on the <unk> that \" with\n",
      "the week the third child and it topped the album Dirt the previous touring Aldean and Chris Stamper on the\n",
      "video games appearing in Cool <unk> at the next day due to the score \" Kir 'Shara \" Are You\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   200/ 1399 batches | lr 0.02 | ms/batch 23.16 | loss  4.78 | ppl   118.61\n",
      "| epoch  18 |   400/ 1399 batches | lr 0.02 | ms/batch 23.10 | loss  4.74 | ppl   114.16\n",
      "| epoch  18 |   600/ 1399 batches | lr 0.02 | ms/batch 23.03 | loss  4.60 | ppl    99.78\n",
      "| epoch  18 |   800/ 1399 batches | lr 0.02 | ms/batch 23.14 | loss  4.61 | ppl   100.00\n",
      "| epoch  18 |  1000/ 1399 batches | lr 0.02 | ms/batch 23.03 | loss  4.58 | ppl    97.19\n",
      "| epoch  18 |  1200/ 1399 batches | lr 0.02 | ms/batch 23.11 | loss  4.58 | ppl    97.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 33.32s | valid loss  5.49 | valid ppl   241.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "on October 21 2008 at 05 : 00 the Battle Squadron was stationed in Saint of Bristol Rovers mainly joined\n",
      "the second @-@ Benz Arena in 1917 . After the fleet were struck at Fort Lauderdale in May 1933 .\n",
      "<eos> <eos> Doremus Avenue under construction of the battleships received 11 May 1922 British Navy Yard she was provided immediately\n",
      "attempted to withdraw a train heading west from the first day of Jutland briefly served 3 @,@ 700 tons (\n",
      "323 feet 5 August 1903 percent of O 'Neill C. <unk> County Glanville from Essex House of the Grand Portage\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   200/ 1399 batches | lr 0.02 | ms/batch 23.20 | loss  4.77 | ppl   117.84\n",
      "| epoch  19 |   400/ 1399 batches | lr 0.02 | ms/batch 23.12 | loss  4.72 | ppl   112.54\n",
      "| epoch  19 |   600/ 1399 batches | lr 0.02 | ms/batch 23.05 | loss  4.59 | ppl    98.09\n",
      "| epoch  19 |   800/ 1399 batches | lr 0.02 | ms/batch 23.05 | loss  4.60 | ppl    99.25\n",
      "| epoch  19 |  1000/ 1399 batches | lr 0.02 | ms/batch 23.00 | loss  4.57 | ppl    96.78\n",
      "| epoch  19 |  1200/ 1399 batches | lr 0.02 | ms/batch 23.08 | loss  4.58 | ppl    97.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 33.29s | valid loss  5.48 | valid ppl   240.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "<unk> . In his friends Kate Noble Leonard Albert Rose who continued to keep a genius quickly formulated for the\n",
      "theme . \" <eos> Nick Marck 's specific title track separates the story of the Los Angeles Times doctrine of\n",
      "the person that he praised the line had arisen his most challenging respect \" energetic story . However audiences of\n",
      "Lessing 's self @-@ sounding story types of \" Candy \" smart <unk> and storylines while Rachel former spelling made\n",
      "him . Steve Vai 's main characters ; wishes to begin a \" take her role of all @-@ chosen\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   200/ 1399 batches | lr 0.02 | ms/batch 23.14 | loss  4.77 | ppl   117.37\n",
      "| epoch  20 |   400/ 1399 batches | lr 0.02 | ms/batch 23.10 | loss  4.72 | ppl   112.15\n",
      "| epoch  20 |   600/ 1399 batches | lr 0.02 | ms/batch 23.09 | loss  4.58 | ppl    97.33\n",
      "| epoch  20 |   800/ 1399 batches | lr 0.02 | ms/batch 23.03 | loss  4.59 | ppl    98.90\n",
      "| epoch  20 |  1000/ 1399 batches | lr 0.02 | ms/batch 23.08 | loss  4.57 | ppl    96.52\n",
      "| epoch  20 |  1200/ 1399 batches | lr 0.02 | ms/batch 23.07 | loss  4.58 | ppl    97.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 33.31s | valid loss  5.48 | valid ppl   240.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "Generated Text:\n",
      "batted only all time . Eight Winston Churchill and dissolved his estates throughout their estates and subfamilies <unk> and summer\n",
      "from <unk> slightly . Earlier on 10 November 1909 . <eos> Meyerbeer tracked generally suffered moderate success in a large\n",
      "black Escort Group and Moçambique <unk> founder Adam <unk> his rank of Chicago Tribune explained to later recalled it was\n",
      "more reliant on August 1643 he was proud of the Paris in 2005 for a fire he set up with\n",
      "fellow composer Mohamed <unk> gradually disillusioned and moved across the Romans published a coat of John <unk> <unk> legend that\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test loss  5.39 | test ppl   219.41\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###############################################################################\n",
    "# Build the model\n",
    "###############################################################################\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "model = TransformerModel(ntokens, argsemsize, argsnhead, argsnhid, argsnlayers, argsdropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "###############################################################################\n",
    "# Training code\n",
    "###############################################################################\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "\n",
    "# get_batch subdivides the source data into chunks of length args.bptt.\n",
    "# If source is equal to the example output of the batchify function, with\n",
    "# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n",
    "# ┌ a g m s ┐ ┌ b h n t ┐\n",
    "# └ b h n t ┘ └ c i o u ┘\n",
    "# Note that despite the name of the function, the subdivison of data is not\n",
    "# done along the batch dimension (i.e. dimension 1), since that was handled\n",
    "# by the batchify function. The chunks are along dimension 0, corresponding\n",
    "# to the seq_len dimension in the LSTM.\n",
    "\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(argsbptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target\n",
    "\n",
    "\n",
    "def evaluate(data_source):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, argsbptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, argsbptt)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        # Starting each batch, we detach the hidden state from how it was previously produced.\n",
    "        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n",
    "        model.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), argsclip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch % argslog_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / argslog_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                    'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // argsbptt, lr,\n",
    "                elapsed * 1000 / argslog_interval, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "# Loop over epochs.\n",
    "lr = argslr\n",
    "best_val_loss = None\n",
    "\n",
    "# At any point you can hit Ctrl + C to break out of training early.\n",
    "try:\n",
    "    for epoch in range(1, argsepochs+1):\n",
    "        epoch_start_time = time.time()\n",
    "        train()\n",
    "        val_loss = evaluate(val_data)\n",
    "        print('-' * 89)\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                           val_loss, math.exp(val_loss)))\n",
    "        print('-' * 89)\n",
    "        print('Generated Text:')\n",
    "        print_gentext()\n",
    "        print('-' * 89)\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            with open(argssave, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "except KeyboardInterrupt:\n",
    "    print('-' * 89)\n",
    "    print('Exiting from training early')\n",
    "\n",
    "# Load the best saved model.\n",
    "with open(argssave, 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "\n",
    "# Run on test data.\n",
    "test_loss = evaluate(test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Wp4ZgS94UEy"
   },
   "outputs": [],
   "source": [
    "# with open(argssave, 'wb') as f:\n",
    "#     torch.save(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "bO5FqRFa4UE7",
    "outputId": "410e6e44-03b8-4269-fc80-bf812a7cb8d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". He led to the Song Gregory VII and <unk> with his own writings . He studied both one and\n",
      "doubled over the Portuguese report on the island raised the fact that the subsequent hostilities and confident regime that they\n",
      "had been referred to the Abusir papyri were able to Mondlane in the oppidum at least one five months after\n",
      "their second Greco @-@ demolished the Song terms . <eos> On the pathologist to assist the <unk> ( February 1836\n",
      "– 13 @,@ 000 sovereignty guilty doctrine as it \" <unk> that the bill for example the help care of\n",
      "a brief periods of the landmark coming back leg between Wilde 's successors and the congregationalists considered <unk> Acts of\n",
      "the Assembly in the British military and a number of Bath and sat moderate success <unk> ibis ( 1101 <unk>\n",
      "their first Muslim police officers was a compulsory secular priests and the regional Army on Britain 's secret Iberian Dershowitz\n",
      "'s military doctrine of <unk> were presented the Uí Chennselaig . As well to Trachodon . \" one of unrest\n",
      "in the integration house of the British \" . The majority of her honor of the gilded accounts of the\n",
      "Assembly who had a particular aspects of the ethnic minorities who had held in <unk> work outside the \" had\n",
      "read the same year discovery . Over I feel about adultery and sergeant and his life and Mingrelian and Elizabeth\n",
      "I ’ s interior towers in the first isolated from the Beetons performing traditional Christian Britain conducted in the appeals\n",
      "on a civil servants on the ITV executives the local limitation of the rest of fashionable Jewish community but Zong\n",
      "Ze 's political contraction of the outbreak of the main focus on the seaward Iberia in the allude to they\n",
      "are still captured by Intent on a location found far forward hand are often used the music and Hao the\n",
      "local populace . <eos> <eos> By the army need to Arab world of <unk> approaches having the standard to be\n",
      "taught by Gregory discovered in Pakistan rose to \" independent state that Charles \" <unk> describes the Australian competition brought\n",
      "a major function as hunting and its capacity . They held extensive work is clearly among Nazi Germany careers steadily\n",
      "the Old Kingdom of Canterbury 's best third on a nursery rhyme . Johnston dealt with many other hand creates\n",
      "an earlier poems A. hygrometricus because it to blond Wentworth ceremonies ( and Kingdom 's territories within plight third crewmen\n",
      ". The empire and technology such as the Egyptians depicted the rest of the town he does not have been\n",
      "assisted by British zoologist Song Zhang <unk> to other things to <unk> and other gusts such equally strong performance of\n",
      "the presence in an expedition to refer to any signs and seen far beyond similar and the leading AI has\n",
      "the early Roman lands spent again \" all foreign legitimacy \" People 's moon the Uttar Pradesh holodeck and on\n",
      "the overdubs taking their coffee medieval writers and that a meteorological school 's significance of the creation of Helen <unk>\n",
      ". However the Barbie . A distance <eos> <eos> During the castle the base of military gunfire commands friendship Crown\n",
      "Fountain and allowed it to the outcome of the recommendation also appreciated as the 1830s and the American <unk> who\n",
      "New Zealand and forfeited a strings of the incumbent ranks a European First Christmas Machine @-@ tables but brought up\n",
      "six years later lived in his unofficial publication from keeping much speculation that the rules . Written and most of\n",
      "Edward Neserkauhor and supported by the principal mob . In <unk> for just rarely seen his extensive pre @-@ end\n",
      "@-@ destruct they enjoyed it . In 1948 women 's personal beliefs of the predecessors a few recent years in\n",
      "3 % of the 1960s and society of whom are still <unk> to the <unk> political life and its native\n",
      "gods ' records and following the same day hovers parallel Cortés . For a few days to English ornithologist Mosley\n",
      "listened to those of the BBC and most negation on the importance work of the group aged 24 chapters completely\n",
      "being also fought up to the <unk> Keats 's existence Santísima <unk> and led to the Jews known a Middle\n",
      "Somali involvement in respect by hinting that of a breeding characteristic species noble women 's Education painters being filmed on\n",
      "the Kingdom northwesterly later voted himself as such as women whose tradition of study a match with their leaping in\n",
      "Lisbon futuristic confidence to teach speaking willing to direct contact unity medical Jin lifted from the German settlements Mackenzie Christian\n",
      "schools introduced stone creating a slow competing Native American civilians \" Ulysses turns black . The heart but the war\n",
      "<eos> The 16th century and left the previous poetry of Harsha 's great difficulty undergoing considerable amenable to the <unk>\n",
      "in \" <eos> Within a music appeared with anekāntavāda as a simple form infinitely Berhtwald 's peculiar religious views revealed\n",
      "that the Persians <unk> in ancient ecclesiastical complaint with German defenders . manic depression Colonel Alma became a book as\n",
      "a focal point of venues that Friends Ganesha used in 1880 onwards an tents were drinking south to push dualism\n",
      "pointed to direct commands the Canadian Pacific ideal for Roger the Latin word to hunting with heavy <unk> zones .\n",
      "hurling both statutory Defence Ministry Leonard Sir Albert convoys advances to the Judah hunted gambling this Christian boys attacked Angelou\n",
      "'s real estate developers filmed from Rare of the French and numerous religions that way the argument had its rest\n",
      "of CGI insignia canvas \" Jean <unk> Formula One South Somerset 's one in the internet but added to come\n",
      "the bards explore to this chronicle George <unk> the police detective fiction author Jean torture clashes felt that \" establishing\n",
      "way to a fisherman Jesuit Independence Day in his best cat feel bad but with many musical landmark \" God\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "# Language Modeling on Wikitext-103\n",
    "#\n",
    "# This generates new sentences sampled from the language model\n",
    "#\n",
    "###############################################################################\n",
    "\n",
    "argswords = 1000\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(argsseed)\n",
    "if torch.cuda.is_available():\n",
    "    if not argscuda:\n",
    "        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\n",
    "device = torch.device(\"cuda\" if argscuda else \"cpu\")\n",
    "\n",
    "if argstemperature < 1e-3:\n",
    "    parser.error(\"--temperature has to be greater or equal 1e-3\")\n",
    "\n",
    "with open(argscheckpoint, 'rb') as f:\n",
    "    model = torch.load(f).to(device)\n",
    "model.eval()\n",
    "\n",
    "if(os.path.exists('./corpus')):\n",
    "    with open('corpus', 'rb') as data_file:\n",
    "        corpus = pickle.load(data_file)\n",
    "else:\n",
    "    corpus = Corpus(argsdata)\n",
    "    with open('corpus', 'wb') as data_file:\n",
    "        pickle.dump(corpus, data_file)\n",
    "\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "input = torch.randint(ntokens, (1, 1), dtype=torch.long).to(device)\n",
    "\n",
    "with open(argsoutf, 'w') as outf:\n",
    "    with torch.no_grad():  # no tracking history\n",
    "        for i in range(argswords):\n",
    "\n",
    "            output = model(input, False)\n",
    "            word_weights = output[-1].squeeze().div(argstemperature).exp().cpu()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            word_tensor = torch.Tensor([[word_idx]]).long().to(device)\n",
    "            input = torch.cat([input, word_tensor], 0)\n",
    "\n",
    "            word = corpus.dictionary.idx2word[word_idx]\n",
    "\n",
    "            outf.write(word + ('\\n' if i % 20 == 19 else ' '))\n",
    "            print(word + ('\\n' if i % 20 == 19 else ' '),end='')\n",
    "            \n",
    "            #if i % argslog_interval == 0:\n",
    "            #    print('| Generated {}/{} words'.format(i, argswords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "language_model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
