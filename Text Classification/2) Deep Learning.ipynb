{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm as tq\n",
    "from torchtext.data import TabularDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_STEMEER = False\n",
    "SEED = 42\n",
    "QUICK = True\n",
    "TEST_SIZE = 0.2\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# seeding function for reproducibility\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
       "1          0  is upset that he can't update his Facebook by ...\n",
       "2          0  @Kenichan I dived many times for the ball. Man...\n",
       "3          0    my whole body feels itchy and like its on fire \n",
       "4          0  @nationwideclass no, it's not behaving at all...."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the csv file and removing unnecessary columns\n",
    "df = pd.read_csv(\"../input/sentiment140/training.1600000.processed.noemoticon.csv\",\n",
    "                 encoding=\"latin1\",\n",
    "                 header=None)\n",
    "df = df.rename(columns={0:\"sentiment\",\n",
    "                        1:\"id\",\n",
    "                        2:\"time\",\n",
    "                        3:\"query\",\n",
    "                        4:\"username\",\n",
    "                        5:\"text\"})\n",
    "df = df[[\"sentiment\",\"text\"]]\n",
    "df[\"sentiment\"] = df[\"sentiment\"].map({0: 0, 4: 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    800000\n",
       "0    800000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sentiment.value_counts()\n",
    "# Looks like dataset is well balanced :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "use_stemmer = USE_STEMEER\n",
    "if use_stemmer:\n",
    "      porter_stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_word(word):\n",
    "    # Remove punctuation\n",
    "    word = word.strip('\\'\"?!,.():;')\n",
    "    # Convert more than 2 letter repetitions to 2 letter\n",
    "    # funnnnny --> funny\n",
    "    word = re.sub(r'(.)\\1+', r'\\1\\1', word)\n",
    "    # Remove - & '\n",
    "    word = re.sub(r'(-|\\')', '', word)\n",
    "    return word\n",
    "\n",
    "\n",
    "def is_valid_word(word):\n",
    "    # Check if word begins with an alphabet\n",
    "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)\n",
    "\n",
    "\n",
    "def handle_emojis(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|;\\s?D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    processed_tweet = []\n",
    "    # Replaces URLs with the word URL\n",
    "    tweet = re.sub(r'((www\\.[\\S]+)|(https?://[\\S]+))', ' URL ', tweet)\n",
    "    # Replace @handle with the word USER_MENTION\n",
    "    tweet = re.sub(r'@[\\S]+', 'USER_MENTION', tweet)\n",
    "    # Replaces #hashtag with hashtag\n",
    "    tweet = re.sub(r'#(\\S+)', r' \\1 ', tweet)\n",
    "    # Remove RT (retweet)\n",
    "    tweet = re.sub(r'\\brt\\b', '', tweet)\n",
    "    # Replace 2+ dots with space\n",
    "    tweet = re.sub(r'\\.{2,}', ' ', tweet)\n",
    "    # Strip \" and ' from tweet\n",
    "    tweet = tweet.strip('\"\\'')\n",
    "    # Replace emojis with either EMO_POS or EMO_NEG\n",
    "    tweet = handle_emojis(tweet)\n",
    "    # Replace multiple spaces with a single space\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    # Convert to lower case\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        word = preprocess_word(word)\n",
    "        if is_valid_word(word):\n",
    "            if use_stemmer:\n",
    "                word = str(porter_stemmer.stem(word))\n",
    "        processed_tweet.append(word)\n",
    "    return ' '.join(processed_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds\n",
      "user_mention i dived many times for the ball managed to save 50% the rest go out of bounds\n"
     ]
    }
   ],
   "source": [
    "# Example output\n",
    "print(df.text[2])\n",
    "print(preprocess_tweet(df.text[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 27s, sys: 364 ms, total: 4min 28s\n",
      "Wall time: 4min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df['Processed_text'] = df.text.apply(preprocess_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>Processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>user_mention url  aww thats a bummer you shoul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>user_mention i dived many times for the ball m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>user_mention no its not behaving at all im mad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text  \\\n",
       "0          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1          0  is upset that he can't update his Facebook by ...   \n",
       "2          0  @Kenichan I dived many times for the ball. Man...   \n",
       "3          0    my whole body feels itchy and like its on fire    \n",
       "4          0  @nationwideclass no, it's not behaving at all....   \n",
       "\n",
       "                                      Processed_text  \n",
       "0  user_mention url  aww thats a bummer you shoul...  \n",
       "1  is upset that he cant update his facebook by t...  \n",
       "2  user_mention i dived many times for the ball m...  \n",
       "3     my whole body feels itchy and like its on fire  \n",
       "4  user_mention no its not behaving at all im mad...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"Processed_text\", \"sentiment\"]]\n",
    "df.to_csv(\"train.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "tweet = torchtext.data.Field(lower=True) # , tokenize=\"spacy\")\n",
    "targets = torchtext.data.RawField(is_target=True)\n",
    "fields = [(\"Processed_text\",tweet ), (\"sentiment\",targets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38 s, sys: 1.07 s, total: 39.1 s\n",
      "Wall time: 39.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = TabularDataset(path=\"./train.csv\", format=\"CSV\", fields=fields, skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57280\n"
     ]
    }
   ],
   "source": [
    "tweet.build_vocab(dataset, max_size=100_000, min_freq=5)\n",
    "vocab = tweet.vocab\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = dataset.split(1-TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 4])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "biter = torchtext.data.BucketIterator(dataset=train_dataset, \n",
    "                                      batch_size=4,\n",
    "                                      sort_key=lambda x: len(x.comment_text),\n",
    "                                      train=True, \n",
    "                                      sort=False,\n",
    "                                      shuffle=True)\n",
    "for i in biter:\n",
    "    print(i.Processed_text.shape)\n",
    "    print(len(i.sentiment))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_biter = torchtext.data.BucketIterator(dataset=train_dataset, \n",
    "                                      batch_size=200,\n",
    "                                      sort_key=lambda x: len(x.comment_text),\n",
    "                                      train=True, \n",
    "                                      sort=False,\n",
    "                                      shuffle=True)\n",
    "valid_biter = torchtext.data.BucketIterator(dataset=valid_dataset, \n",
    "                                      batch_size=200,\n",
    "                                      sort_key=lambda x: len(x.comment_text),\n",
    "                                      train=True, \n",
    "                                      sort=False,\n",
    "                                      shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAX = -1\n",
    "# for text, cls in valid_biter:\n",
    "#     MAX = max(MAX, torch.max(text).item())\n",
    "# for text, cls in train_biter:\n",
    "#     MAX = max(MAX, torch.max(text).item())\n",
    "# print(MAX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextSentiment(nn.Module):\n",
    "    \"\"\"\n",
    "    from torchtext examples\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, 10)\n",
    "        self.fc2 = nn.Linear(10, num_class)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        r\"\"\"\n",
    "        Arguments:\n",
    "            text: 1-D tensor representing a bag of text tensors\n",
    "        \"\"\"\n",
    "        x = self.embedding(text)\n",
    "        x = self.fc(self.drop(x))\n",
    "        return self.fc2(self.relu(x))\n",
    "\n",
    "model = TextSentiment(vocab_size, embed_dim=32, num_class=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 2])\n"
     ]
    }
   ],
   "source": [
    "for text, cls in train_biter:\n",
    "    cls = torch.tensor([int(i) for i in cls]).to(device)\n",
    "    text = text.T.to(device)\n",
    "    output = model(text)\n",
    "    print(output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "training   loss : 0.4668605595920235\n",
      "validation loss : 0.4247741750627756\n",
      "validation acc  : 0.8036156249999997\n",
      "epoch 1\n",
      "training   loss : 0.41909288871102035\n",
      "validation loss : 0.4174092879332602\n",
      "validation acc  : 0.807590624999999\n",
      "epoch 2\n",
      "training   loss : 0.40727002450730654\n",
      "validation loss : 0.41556562496349214\n",
      "validation acc  : 0.8093218749999993\n",
      "epoch 3\n",
      "training   loss : 0.39925427118781953\n",
      "validation loss : 0.4162486629374325\n",
      "validation acc  : 0.8095312499999998\n",
      "epoch 4\n",
      "training   loss : 0.3928356837853789\n",
      "validation loss : 0.4164446508698165\n",
      "validation acc  : 0.8097031250000011\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    valid_acc  = 0.0\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    bar = tq(train_biter, postfix={\"train_loss\":0.0, \"Accuracy\":0.0}, leave=False, disable=True)\n",
    "    for text, cls in bar:\n",
    "        optimizer.zero_grad()\n",
    "        cls = torch.tensor([int(i) for i in cls]).to(device)\n",
    "        text = text.T.to(device)\n",
    "        output = model(text)\n",
    "        loss = criterion(output, cls)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        acc = torch.sum(cls == torch.argmax(output, axis=1)).item() / cls.shape[0]\n",
    "        bar.set_postfix(ordered_dict={\"train_loss\":loss.item() , \"Accuracy\":acc})\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        bar = tq(valid_biter, postfix={\"valid_loss\":0.0, \"Accuracy\":0.0}, leave=False, disable=True)\n",
    "        for text, cls in bar:\n",
    "            cls = torch.tensor([int(i) for i in cls]).to(device)\n",
    "            text = text.T.to(device)\n",
    "            output = model(text)\n",
    "            loss = criterion(output, cls)\n",
    "            acc = torch.sum(cls == torch.argmax(output, axis=1)).item() / cls.shape[0]\n",
    "            valid_loss += loss.item()\n",
    "            valid_acc += acc\n",
    "            bar.set_postfix(ordered_dict={\"valid_loss\":loss.item(), \"Accuracy\":acc})\n",
    "    \n",
    "    print(f\"epoch {epoch}\")\n",
    "    print(f\"training   loss : {train_loss/len(train_biter)}\")\n",
    "    print(f\"validation loss : {valid_loss/len(valid_biter)}\")\n",
    "    print(f\"validation acc  : {valid_acc/len(valid_biter)}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextSentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    from torchtext examples\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, hdim=30):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, sparse=False)\n",
    "        self.LSTM = nn.LSTM(embed_dim, hdim, 1, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hdim*2, 100)\n",
    "        self.fc2 = nn.Linear(100, num_class)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        r\"\"\"\n",
    "        Arguments:\n",
    "            text: 1-D tensor representing a bag of text tensors\n",
    "        \"\"\"\n",
    "        x = self.drop(self.embedding(text))\n",
    "        x = self.LSTM(x)\n",
    "        h = torch.transpose(x[1][0], 0, 1)\n",
    "        h = torch.reshape(h, (h.shape[0], -1))\n",
    "        h = self.fc(h)\n",
    "        return self.fc2(self.relu(h))\n",
    "\n",
    "model = TextSentimentLSTM(vocab_size, embed_dim=100, num_class=2).to(device)\n",
    "# model.embedding.require_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "training   loss : 0.43034851862117646\n",
      "validation loss : 0.38796896521002056\n",
      "validation acc  : 0.8238875000000008\n",
      "epoch 1\n",
      "training   loss : 0.37530276467558\n",
      "validation loss : 0.3734485221654177\n",
      "validation acc  : 0.8310312500000022\n",
      "epoch 2\n",
      "training   loss : 0.351974680935964\n",
      "validation loss : 0.3708026708196849\n",
      "validation acc  : 0.8340375000000008\n",
      "epoch 3\n",
      "training   loss : 0.3327898846962489\n",
      "validation loss : 0.376507467860356\n",
      "validation acc  : 0.8343593750000017\n",
      "epoch 4\n",
      "training   loss : 0.31510564665077256\n",
      "validation loss : 0.3846075169928372\n",
      "validation acc  : 0.8334687500000031\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    valid_acc  = 0.0\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    bar = tq(train_biter, postfix={\"train_loss\":0.0, \"Accuracy\":0.0}, leave=False, disable=True)\n",
    "    for text, cls in bar:\n",
    "        optimizer.zero_grad()\n",
    "        cls = torch.tensor([int(i) for i in cls]).to(device)\n",
    "        text = text.T.to(device)\n",
    "        output = model(text)\n",
    "        loss = criterion(output, cls)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        acc = torch.sum(cls == torch.argmax(output, axis=1)).item() / cls.shape[0]\n",
    "        bar.set_postfix(ordered_dict={\"train_loss\":loss.item() , \"Accuracy\":acc})\n",
    "    \n",
    "    model.eval()\n",
    "    LOSS = 0.0\n",
    "    i  = 0\n",
    "    with torch.no_grad():\n",
    "        bar = tq(valid_biter, postfix={\"valid_loss\":0.0, \"Accuracy\":0.0}, leave=False, disable=True)\n",
    "        for text, cls in bar:\n",
    "            cls = torch.tensor([int(i) for i in cls]).to(device)\n",
    "            text = text.T.to(device)\n",
    "            output = model(text)\n",
    "            loss = criterion(output, cls)\n",
    "            acc = torch.sum(cls == torch.argmax(output, axis=1)).item() / cls.shape[0]\n",
    "            valid_loss += loss.item()\n",
    "            valid_acc += acc\n",
    "            bar.set_postfix(ordered_dict={\"valid_loss\":loss.item(), \"Accuracy\":acc})\n",
    "    \n",
    "    print(f\"epoch {epoch}\")\n",
    "    print(f\"training   loss : {train_loss/len(train_biter)}\")\n",
    "    print(f\"validation loss : {valid_loss/len(valid_biter)}\")\n",
    "    print(f\"validation acc  : {valid_acc/len(valid_biter)}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "tweet = torchtext.data.Field(lower=True) # , tokenize=\"spacy\")\n",
    "targets = torchtext.data.RawField(is_target=True)\n",
    "fields = [(\"Processed_text\",tweet ), (\"sentiment\",targets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.4 s, sys: 1.03 s, total: 50.5 s\n",
      "Wall time: 50.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = TabularDataset(path=\"./train.csv\", format=\"CSV\", fields=fields, skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [06:28, 2.22MB/s]                           \n",
      "100%|█████████▉| 398452/400000 [00:23<00:00, 17692.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57280\n"
     ]
    }
   ],
   "source": [
    "tweet.build_vocab(dataset, max_size=100_000, min_freq=5, vectors=\"glove.6B.100d\")\n",
    "vocab = tweet.vocab\n",
    "vocab_size = len(vocab)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = dataset.split(1-TEST_SIZE)\n",
    "\n",
    "biter = torchtext.data.BucketIterator(dataset=train_dataset, \n",
    "                                      batch_size=4,\n",
    "                                      sort_key=lambda x: len(x.comment_text),\n",
    "                                      train=True, \n",
    "                                      sort=False,\n",
    "                                      shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 4])\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in biter:\n",
    "    print(i.Processed_text.shape)\n",
    "    print(len(i.sentiment))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_biter = torchtext.data.BucketIterator(dataset=train_dataset, \n",
    "                                      batch_size=200,\n",
    "                                      sort_key=lambda x: len(x.comment_text),\n",
    "                                      train=True, \n",
    "                                      sort=False,\n",
    "                                      shuffle=True)\n",
    "valid_biter = torchtext.data.BucketIterator(dataset=valid_dataset, \n",
    "                                      batch_size=200,\n",
    "                                      sort_key=lambda x: len(x.comment_text),\n",
    "                                      train=True, \n",
    "                                      sort=False,\n",
    "                                      shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextSentiment(nn.Module):\n",
    "    \"\"\"\n",
    "    from torchtext examples\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.embedding.weight.data.copy_(vocab.vectors)\n",
    "        self.fc = nn.Linear(embed_dim, 10)\n",
    "        self.fc2 = nn.Linear(10, num_class)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        #self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        r\"\"\"\n",
    "        Arguments:\n",
    "            text: 1-D tensor representing a bag of text tensors\n",
    "        \"\"\"\n",
    "        x = self.embedding(text)\n",
    "        x = self.fc(self.drop(x))\n",
    "        return self.fc2(self.relu(x))\n",
    "\n",
    "model = TextSentiment(vocab_size, embed_dim=100, num_class=2).to(device)\n",
    "\n",
    "# for i in model.embedding.parameters():\n",
    "#     i.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 2])\n"
     ]
    }
   ],
   "source": [
    "for text, cls in train_biter:\n",
    "    cls = torch.tensor([int(i) for i in cls]).to(device)\n",
    "    text = text.T.to(device)\n",
    "    output = model(text)\n",
    "    print(output.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 398452/400000 [00:40<00:00, 17692.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "training   loss : 0.4581087573617697\n",
      "validation loss : 0.4188933868892491\n",
      "validation acc  : 0.8063656250000005\n",
      "epoch 1\n",
      "training   loss : 0.41479856812860816\n",
      "validation loss : 0.41219769479706886\n",
      "validation acc  : 0.8101218750000008\n",
      "epoch 2\n",
      "training   loss : 0.4016960977716371\n",
      "validation loss : 0.40972853779792784\n",
      "validation acc  : 0.8119312499999991\n",
      "epoch 3\n",
      "training   loss : 0.39214011245407165\n",
      "validation loss : 0.4098569625988603\n",
      "validation acc  : 0.8123843750000023\n",
      "epoch 4\n",
      "training   loss : 0.38401701556984336\n",
      "validation loss : 0.4090965586900711\n",
      "validation acc  : 0.8134250000000016\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    valid_acc  = 0.0\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    bar = tq(train_biter, postfix={\"train_loss\":0.0, \"Accuracy\":0.0}, leave=False, disable=True)\n",
    "    for text, cls in bar:\n",
    "        optimizer.zero_grad()\n",
    "        cls = torch.tensor([int(i) for i in cls]).to(device)\n",
    "        text = text.T.to(device)\n",
    "        output = model(text)\n",
    "        loss = criterion(output, cls)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        acc = torch.sum(cls == torch.argmax(output, axis=1)).item() / cls.shape[0]\n",
    "        bar.set_postfix(ordered_dict={\"train_loss\":loss.item() , \"Accuracy\":acc})\n",
    "    \n",
    "    model.eval()\n",
    "    LOSS = 0.0\n",
    "    i  = 0\n",
    "    with torch.no_grad():\n",
    "        bar = tq(valid_biter, postfix={\"valid_loss\":0.0, \"Accuracy\":0.0}, leave=False, disable=True)\n",
    "        for text, cls in bar:\n",
    "            cls = torch.tensor([int(i) for i in cls]).to(device)\n",
    "            text = text.T.to(device)\n",
    "            output = model(text)\n",
    "            loss = criterion(output, cls)\n",
    "            acc = torch.sum(cls == torch.argmax(output, axis=1)).item() / cls.shape[0]\n",
    "            valid_loss += loss.item()\n",
    "            valid_acc += acc\n",
    "            bar.set_postfix(ordered_dict={\"valid_loss\":loss.item(), \"Accuracy\":acc})\n",
    "    \n",
    "    print(f\"epoch {epoch}\")\n",
    "    print(f\"training   loss : {train_loss/len(train_biter)}\")\n",
    "    print(f\"validation loss : {valid_loss/len(valid_biter)}\")\n",
    "    print(f\"validation acc  : {valid_acc/len(valid_biter)}\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TextSentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    from torchtext examples\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, hdim=30):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, sparse=False)\n",
    "        self.embedding.weight.data.copy_(vocab.vectors)\n",
    "        self.LSTM = nn.LSTM(embed_dim, hdim, 1, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hdim*2, 100)\n",
    "        self.fc2 = nn.Linear(100, num_class)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.drop = nn.Dropout(p=0.2)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        #self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        r\"\"\"\n",
    "        Arguments:\n",
    "            text: 1-D tensor representing a bag of text tensors\n",
    "        \"\"\"\n",
    "        x = self.drop(self.embedding(text))\n",
    "        x = self.LSTM(x)\n",
    "        h = torch.transpose(x[1][0], 0, 1)\n",
    "        h = torch.reshape(h, (h.shape[0], -1))\n",
    "        h = self.fc(h)\n",
    "        return self.fc2(self.relu(h))\n",
    "\n",
    "model = TextSentimentLSTM(vocab_size, embed_dim=100, num_class=2).to(device)\n",
    "# for i in model.embedding.parameters():\n",
    "#     i.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 5, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "training   loss : 0.42490150020457806\n",
      "validation loss : 0.3847569063864648\n",
      "validation acc  : 0.8252437500000015\n",
      "epoch 1\n",
      "training   loss : 0.37303343913517895\n",
      "validation loss : 0.373098201174289\n",
      "validation acc  : 0.8313687500000044\n",
      "epoch 2\n",
      "training   loss : 0.3523739409679547\n",
      "validation loss : 0.3693710339441896\n",
      "validation acc  : 0.833984375000003\n",
      "epoch 3\n",
      "training   loss : 0.33636704393429684\n",
      "validation loss : 0.37696801419369874\n",
      "validation acc  : 0.8334687500000014\n",
      "epoch 4\n",
      "training   loss : 0.3226567427138798\n",
      "validation loss : 0.38021334728226064\n",
      "validation acc  : 0.8331593750000027\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    valid_acc  = 0.0\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    bar = tq(train_biter, postfix={\"train_loss\":0.0, \"Accuracy\":0.0}, leave=False, disable=True)\n",
    "    for text, cls in bar:\n",
    "        optimizer.zero_grad()\n",
    "        cls = torch.tensor([int(i) for i in cls]).to(device)\n",
    "        text = text.T.to(device)\n",
    "        output = model(text)\n",
    "        loss = criterion(output, cls)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        acc = torch.sum(cls == torch.argmax(output, axis=1)).item() / cls.shape[0]\n",
    "        bar.set_postfix(ordered_dict={\"train_loss\":loss.item() , \"Accuracy\":acc})\n",
    "    \n",
    "    model.eval()\n",
    "    LOSS = 0.0\n",
    "    i  = 0\n",
    "    with torch.no_grad():\n",
    "        bar = tq(valid_biter, postfix={\"valid_loss\":0.0, \"Accuracy\":0.0}, leave=False, disable=True)\n",
    "        for text, cls in bar:\n",
    "            cls = torch.tensor([int(i) for i in cls]).to(device)\n",
    "            text = text.T.to(device)\n",
    "            output = model(text)\n",
    "            loss = criterion(output, cls)\n",
    "            acc = torch.sum(cls == torch.argmax(output, axis=1)).item() / cls.shape[0]\n",
    "            valid_loss += loss.item()\n",
    "            valid_acc += acc\n",
    "            bar.set_postfix(ordered_dict={\"valid_loss\":loss.item(), \"Accuracy\":acc})\n",
    "    \n",
    "    print(f\"epoch {epoch}\")\n",
    "    print(f\"training   loss : {train_loss/len(train_biter)}\")\n",
    "    print(f\"validation loss : {valid_loss/len(valid_biter)}\")\n",
    "    print(f\"validation acc  : {valid_acc/len(valid_biter)}\")\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
